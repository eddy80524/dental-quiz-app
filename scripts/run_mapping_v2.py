import pandas as pd
import json
import os
import MeCab
import unicodedata
import subprocess
from sklearn.feature_extraction.text import TfidfVectorizer

# --- MeCabã®åˆæœŸåŒ–ï¼ˆè¾æ›¸ã®å ´æ‰€ã‚’è‡ªå‹•ã§æ¢ã™ãƒ­ãƒã‚¹ãƒˆãªæ–¹æ³•ï¼‰ ---
print("--- MeCabã®åˆæœŸåŒ–ã‚’è©¦ã¿ã¾ã™ ---")
try:
    cmd = 'echo `mecab-config --dicdir`"/mecab-ipadic-neologd"'
    path_neologd = subprocess.check_output(cmd, shell=True, text=True).strip()
    
    if os.path.isdir(path_neologd):
        print(f"âœ… æ¨å¥¨è¾æ›¸(neologd)ã‚’'{path_neologd}'ã«ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
        mecab = MeCab.Tagger(f"-Owakati -d {path_neologd}")
    else:
        print("âš ï¸ æ¨å¥¨è¾æ›¸(neologd)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é€šå¸¸ã®è¾æ›¸ã§åˆæœŸåŒ–ã—ã¾ã™ã€‚")
        mecab = MeCab.Tagger("-Owakati")
except Exception:
    print("âš ï¸ è¾æ›¸ã®è‡ªå‹•æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸ã§è©¦ã¿ã¾ã™ã€‚")
    mecab = MeCab.Tagger("-Owakati")
print("âœ… MeCabã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


# --- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
GUIDELINES_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'guidelines_enriched.csv')
QUESTIONS_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'master_questions_final.json')
OUTPUT_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'mapping_result_v2.json')
# â˜…â˜…â˜… å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ±ä¸€ â˜…â˜…â˜…
UPDATED_GUIDELINES_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'guidelines_enriched.csv')
STOPWORDS_PATH = os.path.join(BASE_DIR, 'stopwords.txt') # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 


# --- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿å‡¦ç†ã‚’è¿½åŠ  ---
stopwords = set()
if os.path.exists(STOPWORDS_PATH):
    with open(STOPWORDS_PATH, 'r', encoding='utf-8') as f:
        stopwords = {line.strip().lower() for line in f if line.strip()}
print(f"âœ… {len(stopwords)}èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")


# --- é–¢æ•°å®šç¾© ---
def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€å°æ–‡å­—åŒ–ï¼‰"""
    return unicodedata.normalize('NFKC', str(text)).lower()

def tokenize(text):
    """MeCabã‚’ä½¿ã£ã¦åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‚’æŠ½å‡ºã—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–"""
    node = mecab.parseToNode(normalize_text(text))
    words = []
    while node:
        word = node.surface
        features = node.feature.split(',')
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œãšã€å“è©ãŒå¯¾è±¡ã®å ´åˆã®ã¿è¿½åŠ 
        if word not in stopwords and features[0] in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
            words.append(word)
        node = node.next
    return " ".join(words)


# --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
print("--- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ ---")
guidelines_df = pd.read_csv(GUIDELINES_PATH)
guidelines_df['id'] = range(len(guidelines_df))

with open(QUESTIONS_PATH, 'r', encoding='utf-8') as f:
    q_data = json.load(f)
    questions_list = q_data.get('questions', [])
    cases_dict = q_data.get('cases', {})

questions_df = pd.DataFrame(questions_list)

# --- å…¨å•é¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç† ---
print("--- å…¨å•é¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ä¸­ ---")
questions_df['full_text'] = questions_df.apply(
    lambda row: normalize_text(
        str(row.get('question', '')) + ' ' + 
        ' '.join(map(str, row.get('choices', []) if row.get('choices') is not None else []))
    ), axis=1
)

# --- åˆæœŸãƒãƒƒãƒ”ãƒ³ã‚° ---
print("--- ãƒ•ã‚§ãƒ¼ã‚º1a: åˆæœŸãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­ ---")
question_to_guideline_map_initial = {}
for _, question in questions_df.iterrows():
    search_text = question['full_text']
    best_score, best_guideline_id = 0, -1
    for _, guideline in guidelines_df.iterrows():
        keywords = str(guideline['keywords']).split(';')
        score = sum(1 for kw in keywords if kw and normalize_text(kw) in search_text)
        if score > best_score:
            best_score, best_guideline_id = score, int(guideline['id'])
    question_to_guideline_map_initial[question['number']] = best_guideline_id

# --- TF-IDFã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ ---
print("--- ãƒ•ã‚§ãƒ¼ã‚º1b: TF-IDFã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•æ‹¡å¼µä¸­ ---")
mapped_questions = {k: v for k, v in question_to_guideline_map_initial.items() if v != -1}
questions_df['guideline_id'] = questions_df['number'].map(mapped_questions)

corpus_by_guideline = questions_df.dropna(subset=['guideline_id']).groupby('guideline_id')['full_text'].apply(' '.join).tolist()
guideline_ids_in_corpus = questions_df.dropna(subset=['guideline_id']).groupby('guideline_id')['full_text'].apply(' '.join).index.tolist()

if corpus_by_guideline:
    tokenized_corpus = [tokenize(doc) for doc in corpus_by_guideline]
    
    # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´æ¸ˆã¿ ---
    vectorizer = TfidfVectorizer(max_features=3000, max_df=0.5, min_df=1)
    tfidf_matrix = vectorizer.fit_transform(tokenized_corpus)
    feature_names = vectorizer.get_feature_names_out()

    for i, guideline_id in enumerate(guideline_ids_in_corpus):
        feature_index = tfidf_matrix[i,:].nonzero()[1]
        tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])
        
        # --- æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’20ã«å¢—åŠ  ---
        top_keywords = [feature_names[i] for i, s in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]]
        
        existing_keywords = guidelines_df.loc[guidelines_df['id'] == guideline_id, 'keywords'].iloc[0]
        existing_set = set(str(existing_keywords).split(';')) if pd.notna(existing_keywords) else set()
        new_set = existing_set.union(set(top_keywords))
        guidelines_df.loc[guidelines_df['id'] == guideline_id, 'keywords'] = ';'.join(filter(None, new_set))

print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DBã‚’å¼·åŒ–ã—ã¾ã—ãŸã€‚")
guidelines_df.to_csv(UPDATED_GUIDELINES_PATH, index=False)
print(f"âœ… å¼·åŒ–ç‰ˆDBã‚’ '{UPDATED_GUIDELINES_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# --- å¼·åŒ–æ¸ˆã¿DBã§å†ãƒãƒƒãƒ”ãƒ³ã‚° ---
print("--- ãƒ•ã‚§ãƒ¼ã‚º2: å¼·åŒ–æ¸ˆã¿DBã§å†ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­ ---")
question_to_guideline_map_final = {}
for _, question in questions_df.iterrows():
    search_text = question['full_text']
    best_score, best_guideline_id = 0, -1
    for _, guideline in guidelines_df.iterrows():
        keywords = str(guideline['keywords']).split(';')
        score = sum(1 for kw in keywords if kw and normalize_text(kw) in search_text)
        if score > best_score:
            best_score, best_guideline_id = score, int(guideline['id'])
    question_to_guideline_map_final[question['number']] = best_guideline_id

# --- æœ€çµ‚çµæœã®ä¿å­˜ ---
with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
    json.dump(question_to_guideline_map_final, f, ensure_ascii=False, indent=2)

print(f"ğŸ‰ å…¨å·¥ç¨‹å®Œäº†ï¼ æœ€çµ‚ãƒãƒƒãƒ”ãƒ³ã‚°çµæœã‚’ '{OUTPUT_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")