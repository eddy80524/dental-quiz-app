import os
import pandas as pd
import unicodedata
from sentence_transformers import SentenceTransformer, util
import torch

print("--- ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™ ---")


# --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
GUIDELINES_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'guidelines_enriched.csv')
EXPERT_KEYWORDS_PATH = os.path.join(BASE_DIR, 'expert_keywords.txt')
# â˜…â˜…â˜… å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ±ä¸€ â˜…â˜…â˜…
OUTPUT_PATH = os.path.join(BASE_DIR, '..', 'my_llm_app', 'data', 'guidelines_enriched.csv')
# --- 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
try:
    print("--- ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ ---")
    guidelines_df = pd.read_csv(GUIDELINES_PATH)
    with open(EXPERT_KEYWORDS_PATH, 'r', encoding='utf-8') as f:
        expert_keywords = [line.strip() for line in f if line.strip()]
    print(f"âœ… ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ {len(guidelines_df)}ä»¶ã€å°‚é–€ç”¨èª {len(expert_keywords)}èªã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
except FileNotFoundError as e:
    print(f"âŒã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ {e}")
    exit()

# --- 3. AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ ---
print("--- AIè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆåˆå›ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰ ---")
# æ—¥æœ¬èªã«å¼·ãã€æ„å‘³ã®é¡ä¼¼åº¦æ¤œç´¢ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
model = SentenceTransformer('cl-tohoku/bert-base-japanese-whole-word-masking')
print("âœ… AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ã€‚")

# --- 4. åŸºæº–é …ç›®ã®ã€Œæ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’ä½œæˆ ---
print("--- åŸºæº–é …ç›®ã®æ–‡è„ˆã‚’æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ä¸­ ---")
# å„åŸºæº–é …ç›®ã‚’ä»£è¡¨ã™ã‚‹ã€Œæ–‡è„ˆãƒ†ã‚­ã‚¹ãƒˆã€ã‚’ä½œæˆ
guidelines_df['context_text'] = guidelines_df.apply(
    lambda row: ' '.join(map(str, [
        row['chapter'], row['daikoumoku'], row['chukoumoku'], 
        row['shoukoumoku'], row['remarks']
    ])).replace('nan', ''), # nanï¼ˆæ¬ æå€¤ï¼‰ã‚’ç©ºæ–‡å­—ã«ç½®æ›
    axis=1
)
# å…¨ã¦ã®æ–‡è„ˆãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åº¦ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆé«˜é€Ÿï¼‰
guideline_embeddings = model.encode(guidelines_df['context_text'].tolist(), convert_to_tensor=True)
print("âœ… æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆå®Œäº†ã€‚")

# --- 5. å°‚é–€ç”¨èªã®åˆ†é¡ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ  ---
print("--- å°‚é–€ç”¨èªã®åˆ†é¡ã‚’é–‹å§‹ã—ã¾ã™ ---")
classified_count = 0
for keyword in expert_keywords:
    # å°‚é–€ç”¨èªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    keyword_embedding = model.encode(keyword, convert_to_tensor=True)
    
    # å…¨ã¦ã®åŸºæº–é …ç›®ã¨ã®é¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã‚’è¨ˆç®—
    cosine_scores = util.cos_sim(keyword_embedding, guideline_embeddings)
    
    # æœ€ã‚‚é¡ä¼¼åº¦ãŒé«˜ã„é …ç›®ã®IDã‚’å–å¾—
    best_match_index = torch.argmax(cosine_scores).item()
    
    # è©²å½“ã™ã‚‹åŸºæº–é …ç›®ã®æ—¢å­˜ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
    existing_keywords_str = guidelines_df.loc[best_match_index, 'keywords']
    existing_keywords = set(str(existing_keywords_str).split(';')) if pd.notna(existing_keywords_str) else set()
    
    # ã¾ã ç™»éŒ²ã•ã‚Œã¦ã„ãªã‘ã‚Œã°è¿½åŠ 
    if keyword not in existing_keywords:
        existing_keywords.add(keyword)
        guidelines_df.loc[best_match_index, 'keywords'] = ';'.join(filter(None, sorted(list(existing_keywords))))
        print(f"  - ã€Œ{keyword}ã€ã‚’é …ç›®ID {best_match_index} ({guidelines_df.loc[best_match_index, 'chukoumoku']}) ã«åˆ†é¡ã—ã¾ã—ãŸã€‚")
        classified_count += 1

print(f"âœ… {classified_count}èªã®å°‚é–€ç”¨èªã‚’åˆ†é¡ãƒ»è¿½åŠ ã—ã¾ã—ãŸã€‚")

# --- 6. æœ€çµ‚çµæœã®ä¿å­˜ ---
guidelines_df.drop(columns=['context_text'], inplace=True)
guidelines_df.to_csv(OUTPUT_PATH, index=False)
print(f"ğŸ‰ å…¨å·¥ç¨‹å®Œäº†ï¼ æœ€çµ‚ç‰ˆã®DBã‚’ '{OUTPUT_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")