"""
åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°åˆ†æã¨æœ€é©åŒ–ææ¡ˆ
"""

import datetime
from typing import Dict, Any, List
from complete_migration_system import CompleteMigrationSystem


class AnalyticsSummaryAnalyzer:
    """åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–åˆ†æ"""
    
    def __init__(self):
        migration_system = CompleteMigrationSystem()
        self.db = migration_system.db
    
    def analyze_analytics_collections(self) -> Dict[str, Any]:
        """åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°åˆ†æ"""
        analysis = {
            "analyzed_at": datetime.datetime.now(),
            "collections": {},
            "redundancy_analysis": {},
            "optimization_recommendations": {}
        }
        
        # åˆ†æå¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        collections_to_analyze = [
            'analytics_summary',
            'daily_analytics_summary', 
            'weekly_analytics_summary',
            'monthly_analytics_summary',
            'daily_active_users',
            'daily_learning_logs',
            'daily_engagement_summary'
        ]
        
        for collection_name in collections_to_analyze:
            collection_analysis = self._analyze_single_collection(collection_name)
            analysis["collections"][collection_name] = collection_analysis
        
        # å†—é•·æ€§åˆ†æ
        analysis["redundancy_analysis"] = self._analyze_redundancy(analysis["collections"])
        
        # æœ€é©åŒ–ææ¡ˆ
        analysis["optimization_recommendations"] = self._generate_optimization_recommendations(analysis)
        
        return analysis
    
    def _analyze_single_collection(self, collection_name: str) -> Dict[str, Any]:
        """å˜ä¸€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ"""
        result = {
            "name": collection_name,
            "exists": False,
            "document_count": 0,
            "sample_data": None,
            "data_structure": [],
            "date_range": {},
            "user_coverage": 0,
            "storage_estimate": 0,
            "necessity_score": 0
        }
        
        try:
            collection_ref = self.db.collection(collection_name)
            docs = list(collection_ref.stream())
            
            if not docs:
                return result
                
            result["exists"] = True
            result["document_count"] = len(docs)
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿åˆ†æ
            sample_doc = docs[0].to_dict()
            result["sample_data"] = sample_doc
            result["data_structure"] = list(sample_doc.keys())
            
            # æ—¥ä»˜ç¯„å›²åˆ†æ
            dates = []
            users = set()
            
            for doc in docs:
                data = doc.to_dict()
                
                # æ—¥ä»˜æƒ…å ±å–å¾—
                if 'date' in data:
                    dates.append(data['date'])
                elif 'timestamp' in data:
                    dates.append(str(data['timestamp'])[:10])
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—
                if 'uid' in data:
                    users.add(data['uid'])
                elif 'userId' in data:
                    users.add(data['userId'])
            
            if dates:
                result["date_range"] = {
                    "earliest": min(dates),
                    "latest": max(dates),
                    "span_days": len(set(dates))
                }
            
            result["user_coverage"] = len(users)
            
            # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¦‹ç©ã‚‚ã‚Šï¼ˆãŠãŠã‚ˆãï¼‰
            avg_doc_size = len(str(sample_doc)) * 1.5  # JSON + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¦‹ç©ã‚‚ã‚Š
            result["storage_estimate"] = int(avg_doc_size * len(docs))
            
            # å¿…è¦æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            result["necessity_score"] = self._calculate_necessity_score(collection_name, result)
            
        except Exception as e:
            result["error"] = str(e)
        
        return result
    
    def _calculate_necessity_score(self, collection_name: str, analysis: Dict[str, Any]) -> int:
        """å¿…è¦æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # åŸºæœ¬å­˜åœ¨ã‚¹ã‚³ã‚¢
        if analysis["exists"]:
            score += 20
        
        # ãƒ‡ãƒ¼ã‚¿é‡ã‚¹ã‚³ã‚¢
        doc_count = analysis["document_count"]
        if doc_count > 100:
            score += 30
        elif doc_count > 10:
            score += 20
        elif doc_count > 0:
            score += 10
        
        # æœ€æ–°æ€§ã‚¹ã‚³ã‚¢
        if analysis["date_range"]:
            latest_date = analysis["date_range"].get("latest", "")
            if latest_date >= "2025-08-25":  # æœ€è¿‘4æ—¥ä»¥å†…
                score += 25
            elif latest_date >= "2025-08-20":  # æœ€è¿‘9æ—¥ä»¥å†…
                score += 15
            elif latest_date >= "2025-08-01":  # ä»Šæœˆ
                score += 5
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢
        user_count = analysis["user_coverage"]
        if user_count > 10:
            score += 15
        elif user_count > 5:
            score += 10
        elif user_count > 0:
            score += 5
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®é‡è¦åº¦
        if collection_name in ['analytics_summary']:
            score += 10  # ä¸»è¦ãªåˆ†æãƒ‡ãƒ¼ã‚¿
        elif collection_name in ['daily_active_users', 'daily_learning_logs']:
            score += 5   # æœ‰ç”¨ã ãŒä»£æ›¿å¯èƒ½
        
        return min(score, 100)
    
    def _analyze_redundancy(self, collections: Dict[str, Any]) -> Dict[str, Any]:
        """å†—é•·æ€§åˆ†æ"""
        redundancy = {
            "duplicate_data": [],
            "overlapping_metrics": [],
            "consolidation_opportunities": []
        }
        
        # å­˜åœ¨ã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        existing_collections = {name: data for name, data in collections.items() 
                              if data.get("exists", False)}
        
        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º
        for name1, data1 in existing_collections.items():
            for name2, data2 in existing_collections.items():
                if name1 < name2:  # é‡è¤‡ãƒã‚§ãƒƒã‚¯å›é¿
                    fields1 = set(data1.get("data_structure", []))
                    fields2 = set(data2.get("data_structure", []))
                    
                    overlap = fields1.intersection(fields2)
                    if len(overlap) > 2:  # 2ã¤ä»¥ä¸Šã®å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                        redundancy["overlapping_metrics"].append({
                            "collections": [name1, name2],
                            "overlapping_fields": list(overlap),
                            "overlap_percentage": len(overlap) / max(len(fields1), len(fields2)) * 100
                        })
        
        # çµ±åˆæ©Ÿä¼šã®ç‰¹å®š
        daily_collections = [name for name in existing_collections.keys() if 'daily' in name]
        if len(daily_collections) > 1:
            redundancy["consolidation_opportunities"].append({
                "type": "daily_collections_merge",
                "collections": daily_collections,
                "benefit": "çµ±ä¸€ã•ã‚ŒãŸæ—¥æ¬¡åˆ†æãƒ‡ãƒ¼ã‚¿æ§‹é€ "
            })
        
        return redundancy
    
    def _generate_optimization_recommendations(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            "immediate_actions": [],
            "consolidation_plan": [],
            "cost_savings": {},
            "migration_to_optimized": []
        }
        
        collections = analysis["collections"]
        
        # å³åº§å‰Šé™¤æ¨å¥¨
        for name, data in collections.items():
            if data.get("exists", False):
                necessity_score = data.get("necessity_score", 0)
                doc_count = data.get("document_count", 0)
                
                if necessity_score < 30:
                    recommendations["immediate_actions"].append({
                        "action": "delete",
                        "collection": name,
                        "reason": f"å¿…è¦æ€§ã‚¹ã‚³ã‚¢ä½ã„({necessity_score}/100)",
                        "savings": f"{doc_count}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤"
                    })
                elif necessity_score < 60:
                    recommendations["immediate_actions"].append({
                        "action": "review",
                        "collection": name,
                        "reason": f"å¿…è¦æ€§è¦æ¤œè¨({necessity_score}/100)",
                        "suggestion": "ä»£æ›¿æ‰‹æ®µã¨ã®æ¯”è¼ƒæ¤œè¨"
                    })
        
        # æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ç§»è¡Œæ¨å¥¨
        existing_collections = [name for name, data in collections.items() if data.get("exists")]
        if existing_collections:
            recommendations["migration_to_optimized"] = [
                {
                    "target": "users.statsçµ±åˆ",
                    "source_collections": existing_collections,
                    "benefit": "å˜ä¸€ã®æœ€é©åŒ–çµ±è¨ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ",
                    "implementation": "enhanced_firestore_optimizerã®statsæ©Ÿèƒ½æ´»ç”¨"
                },
                {
                    "target": "weekly_rankingsæ´»ç”¨",
                    "source_collections": [c for c in existing_collections if 'daily' in c or 'weekly' in c],
                    "benefit": "äº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿",
                    "implementation": "OptimizedWeeklyRankingSystemã®æ©Ÿèƒ½æ‹¡å¼µ"
                }
            ]
        
        # ã‚³ã‚¹ãƒˆå‰Šæ¸›è¦‹ç©ã‚‚ã‚Š
        total_docs = sum(data.get("document_count", 0) for data in collections.values() if data.get("exists"))
        total_storage = sum(data.get("storage_estimate", 0) for data in collections.values() if data.get("exists"))
        
        recommendations["cost_savings"] = {
            "documents_removable": total_docs,
            "storage_savings_bytes": total_storage,
            "monthly_read_cost_reduction": f"${total_docs * 0.0000006:.4f}",  # Firestoreèª­ã¿å–ã‚Šã‚³ã‚¹ãƒˆæ¦‚ç®—
            "monthly_write_cost_reduction": f"${total_docs * 0.0000018:.4f}"   # Firestoreæ›¸ãè¾¼ã¿ã‚³ã‚¹ãƒˆæ¦‚ç®—
        }
        
        return recommendations
    
    def generate_cleanup_script(self, analysis: Dict[str, Any]) -> List[str]:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ"""
        recommendations = analysis["optimization_recommendations"]
        
        commands = [
            "#!/bin/bash",
            "# =====================================",
            "# åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
            "# =====================================",
            "",
            "echo 'ğŸ” åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–é–‹å§‹'",
            "",
            "# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ",
            "python -c \"",
            "from complete_migration_system import CompleteMigrationSystem",
            "migration = CompleteMigrationSystem()",
            "backup_id = migration.backup_existing_data()",
            "print(f'ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_id}')",
            "\"",
            ""
        ]
        
        # å‰Šé™¤å¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†
        delete_actions = [action for action in recommendations["immediate_actions"] 
                         if action["action"] == "delete"]
        
        if delete_actions:
            commands.extend([
                "# 2. ä½å¿…è¦æ€§ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤",
                "python -c \"",
                "from complete_migration_system import CompleteMigrationSystem",
                "migration = CompleteMigrationSystem()",
                "db = migration.db",
                ""
            ])
            
            for action in delete_actions:
                collection = action["collection"]
                commands.extend([
                    f"# {collection}å‰Šé™¤",
                    f"docs = list(db.collection('{collection}').stream())",
                    f"print(f'ğŸ—‘ï¸ å‰Šé™¤å¯¾è±¡: {collection} {{len(docs)}}ä»¶')",
                    "for doc in docs:",
                    "    doc.reference.delete()",
                    f"print(f'âœ… {collection}å‰Šé™¤å®Œäº†')",
                    ""
                ])
            
            commands.extend(["\"\n"])
        
        # æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        commands.extend([
            "# 3. å‰Šé™¤å¾Œç¢ºèª",
            "python -c \"",
            "from complete_migration_system import CompleteMigrationSystem",
            "migration = CompleteMigrationSystem()",
            "db = migration.db",
            "",
            "collections_to_check = [" + 
            ", ".join([f"'{action['collection']}'" for action in delete_actions]) + "]",
            "",
            "for collection_name in collections_to_check:",
            "    remaining = list(db.collection(collection_name).limit(1).stream())",
            "    if remaining:",
            "        print(f'âš ï¸ {collection_name}: ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã£ã¦ã„ã¾ã™')",
            "    else:",
            "        print(f'âœ… {collection_name}: å‰Šé™¤å®Œäº†ç¢ºèª')",
            "\"",
            "",
            "echo 'ğŸ‰ åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–å®Œäº†'"
        ])
        
        return commands


def main():
    """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
    analyzer = AnalyticsSummaryAnalyzer()
    
    print("ğŸ“Š åˆ†æé–¢é€£ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°åˆ†æ")
    print("=" * 60)
    
    # è©³ç´°åˆ†æå®Ÿè¡Œ
    analysis = analyzer.analyze_analytics_collections()
    
    print(f"\nğŸ“‹ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åˆ¥è©³ç´°åˆ†æ:")
    for name, data in analysis["collections"].items():
        if data.get("exists", False):
            print(f"\nâœ… {name}:")
            print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {data['document_count']}ä»¶")
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚«ãƒãƒ¬ãƒƒã‚¸: {data['user_coverage']}å")
            print(f"   å¿…è¦æ€§ã‚¹ã‚³ã‚¢: {data['necessity_score']}/100")
            if data.get("date_range"):
                date_range = data["date_range"]
                print(f"   ãƒ‡ãƒ¼ã‚¿æœŸé–“: {date_range['earliest']} ï½ {date_range['latest']} ({date_range['span_days']}æ—¥é–“)")
            print(f"   ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {data['data_structure'][:3]}..." if len(data['data_structure']) > 3 else f"   ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {data['data_structure']}")
        else:
            print(f"\nâŒ {name}: å­˜åœ¨ã—ãªã„")
    
    print(f"\nğŸ”„ å†—é•·æ€§åˆ†æ:")
    redundancy = analysis["redundancy_analysis"]
    
    if redundancy["overlapping_metrics"]:
        print("é‡è¤‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for overlap in redundancy["overlapping_metrics"]:
            collections = " & ".join(overlap["collections"])
            print(f"   {collections}: {overlap['overlap_percentage']:.1f}%é‡è¤‡")
            print(f"   å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {overlap['overlapping_fields']}")
    else:
        print("é‡è¤‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: ãªã—")
    
    if redundancy["consolidation_opportunities"]:
        print("çµ±åˆæ©Ÿä¼š:")
        for opportunity in redundancy["consolidation_opportunities"]:
            print(f"   {opportunity['type']}: {opportunity['collections']}")
            print(f"   åŠ¹æœ: {opportunity['benefit']}")
    
    print(f"\nğŸ’¡ æœ€é©åŒ–æ¨å¥¨äº‹é …:")
    recommendations = analysis["optimization_recommendations"]
    
    print("å³åº§å¯¾å¿œ:")
    for action in recommendations["immediate_actions"]:
        action_icon = "ğŸ—‘ï¸" if action["action"] == "delete" else "ğŸ”"
        print(f"   {action_icon} {action['collection']}: {action['reason']}")
    
    print(f"\nğŸ’° ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ:")
    savings = recommendations["cost_savings"]
    print(f"   å‰Šé™¤å¯èƒ½ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {savings['documents_removable']}ä»¶")
    print(f"   ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å‰Šæ¸›: {savings['storage_savings_bytes']:,}ãƒã‚¤ãƒˆ")
    print(f"   æœˆæ¬¡èª­ã¿å–ã‚Šã‚³ã‚¹ãƒˆå‰Šæ¸›: {savings['monthly_read_cost_reduction']}")
    print(f"   æœˆæ¬¡æ›¸ãè¾¼ã¿ã‚³ã‚¹ãƒˆå‰Šæ¸›: {savings['monthly_write_cost_reduction']}")
    
    print(f"\nğŸš€ æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ç§»è¡Œæ¨å¥¨:")
    for migration in recommendations["migration_to_optimized"]:
        print(f"   {migration['target']}: {migration['benefit']}")
        print(f"   å¯¾è±¡: {migration['source_collections']}")
        print(f"   å®Ÿè£…: {migration['implementation']}")
        print()
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
    script = analyzer.generate_cleanup_script(analysis)
    
    with open("/tmp/cleanup_analytics_collections.sh", "w", encoding="utf-8") as f:
        f.write("\n".join(script))
    
    print(f"ğŸ—‚ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ /tmp/cleanup_analytics_collections.sh ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # ç·åˆæ¨å¥¨äº‹é …
    delete_count = len([a for a in recommendations["immediate_actions"] if a["action"] == "delete"])
    review_count = len([a for a in recommendations["immediate_actions"] if a["action"] == "review"])
    
    print(f"\nğŸ¯ ç·åˆæ¨å¥¨:")
    print(f"   å³åº§å‰Šé™¤æ¨å¥¨: {delete_count}ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    print(f"   è¦æ¤œè¨: {review_count}ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³") 
    print(f"   æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã«ã‚ˆã‚Šã€åˆ†ææ©Ÿèƒ½ã‚’åŠ¹ç‡åŒ–")


if __name__ == "__main__":
    main()
