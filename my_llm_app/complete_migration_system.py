"""
å®Œå…¨è‡ªå‹•åŒ–Firestoreç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–æ§‹é€ ã«å®‰å…¨ã«ç§»è¡Œ

ç‰¹å¾´:
1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ä»˜ã
2. æ®µéšçš„ç§»è¡Œ
3. ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œ
4. é€²æ—ç›£è¦–
5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

import datetime
import json
import time
from typing import Dict, Any, List, Optional
from enhanced_firestore_optimizer import EnhancedFirestoreOptimizer
from optimized_weekly_ranking import OptimizedWeeklyRankingSystem
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CompleteMigrationSystem:
    """å®Œå…¨è‡ªå‹•åŒ–ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.optimizer = EnhancedFirestoreOptimizer()
        self.ranking_system = OptimizedWeeklyRankingSystem()
        self.db = self.optimizer.db
        self.migration_log = []
    
    def backup_existing_data(self, backup_id: str = None) -> str:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        if backup_id is None:
            backup_id = f"backup_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹: {backup_id}")
            
            backup_data = {
                "backup_id": backup_id,
                "created_at": datetime.datetime.now().isoformat(),
                "users": {},
                "user_cards": {},
                "weekly_rankings": {}
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            users_ref = self.db.collection("users")
            users_docs = list(users_ref.stream())
            
            for user_doc in users_docs:
                user_data = user_doc.to_dict()
                backup_data["users"][user_doc.id] = self._serialize_firestore_data(user_data)
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚«ãƒ¼ãƒ‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                cards_ref = user_doc.reference.collection("userCards")
                cards_docs = list(cards_ref.stream())
                
                user_cards = {}
                for card_doc in cards_docs:
                    card_data = card_doc.to_dict()
                    user_cards[card_doc.id] = self._serialize_firestore_data(card_data)
                
                if user_cards:
                    backup_data["user_cards"][user_doc.id] = user_cards
            
            # é€±é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            rankings_ref = self.db.collection("weekly_rankings")
            rankings_docs = list(rankings_ref.stream())
            
            for ranking_doc in rankings_docs:
                ranking_data = ranking_doc.to_dict()
                backup_data["weekly_rankings"][ranking_doc.id] = self._serialize_firestore_data(ranking_data)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’Firestoreã«ä¿å­˜
            backup_ref = self.db.collection("migration_backups").document(backup_id)
            backup_ref.set({
                "backup_id": backup_id,
                "created_at": datetime.datetime.now(),
                "data_size": len(json.dumps(backup_data)),
                "collections_backed_up": ["users", "user_cards", "weekly_rankings"],
                "status": "completed"
            })
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚‚ä¿å­˜ï¼ˆå†—é•·æ€§ç¢ºä¿ï¼‰
            self._save_backup_to_file(backup_data, backup_id)
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_id}")
            return backup_id
            
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _serialize_firestore_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Firestoreãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        serialized = {}
        
        for key, value in data.items():
            if isinstance(value, datetime.datetime):
                serialized[key] = value.isoformat()
            elif isinstance(value, dict):
                serialized[key] = self._serialize_firestore_data(value)
            elif isinstance(value, list):
                serialized[key] = [
                    self._serialize_firestore_data(item) if isinstance(item, dict) else item
                    for item in value
                ]
            else:
                serialized[key] = value
        
        return serialized
    
    def _save_backup_to_file(self, backup_data: Dict[str, Any], backup_id: str):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            filename = f"/tmp/{backup_id}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.warning(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {e}")
    
    def _get_gakushi_permission(self, uid: str) -> bool:
        """user_permissionsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å­¦å£«æ¨©é™ã‚’å–å¾—"""
        try:
            # user_permissionsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¨©é™ãƒã‚§ãƒƒã‚¯
            permission_doc = self.db.collection("user_permissions").document(uid).get()
            if permission_doc.exists:
                permission_data = permission_doc.to_dict()
                return permission_data.get("can_access_gakushi", False)
            
            # user_permissionsãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueï¼ˆæ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®é…æ…®ï¼‰
            logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ã®å­¦å£«æ¨©é™ãƒ‡ãƒ¼ã‚¿ãªã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueã‚’è¨­å®š")
            return True
            
        except Exception as e:
            logger.warning(f"å­¦å£«æ¨©é™å–å¾—ã‚¨ãƒ©ãƒ¼ (uid: {uid[:8]}): {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Trueã‚’è¿”ã™ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
            return True
    
    def migrate_user_data_safe(self, uid: str) -> bool:
        """å®‰å…¨ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ"""
        try:
            logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ç§»è¡Œé–‹å§‹")
            
            # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            user_doc = self.db.collection("users").document(uid).get()
            if not user_doc.exists:
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            user_data = user_doc.to_dict()
            if not user_data.get("email"):
                logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ã¯ç„¡åŠ¹ã§ã™ï¼ˆemailãªã—ï¼‰")
                return False
            
            # 2. æ—¢å­˜userCardsã®å–å¾—ã¨å¤‰æ›
            old_cards_ref = user_doc.reference.collection("userCards")
            old_cards_docs = list(old_cards_ref.stream())
            
            logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} æ—¢å­˜ã‚«ãƒ¼ãƒ‰æ•°: {len(old_cards_docs)}")
            
            # 3. æœ€é©åŒ–ã•ã‚ŒãŸstudy_cardsã«ç§»è¡Œ
            migrated_cards = 0
            batch_size = 100
            
            for i in range(0, len(old_cards_docs), batch_size):
                batch_cards = old_cards_docs[i:i + batch_size]
                
                batch = self.db.batch()
                
                for card_doc in batch_cards:
                    old_card_data = card_doc.to_dict()
                    
                    # æœ€é©åŒ–æ§‹é€ ã«å¤‰æ›
                    optimized_card = self._convert_to_optimized_card(uid, card_doc.id, old_card_data)
                    
                    # æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    new_card_id = f"{uid}_{card_doc.id}"
                    new_card_ref = self.db.collection("study_cards").document(new_card_id)
                    batch.set(new_card_ref, optimized_card)
                    
                    migrated_cards += 1
                
                batch.commit()
                logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ã‚«ãƒ¼ãƒ‰ç§»è¡Œ: {migrated_cards}/{len(old_cards_docs)}")
            
            # 4. å­¦å£«æ¨©é™ã®å–å¾—
            gakushi_permission = self._get_gakushi_permission(uid)
            
            # 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆã®è¨ˆç®—ã¨æ›´æ–°
            stats = self.optimizer.calculate_user_statistics_batch(uid)
            
            # 6. æœ€é©åŒ–ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            optimized_user_data = {
                "email": user_data.get("email", ""),
                "nickname": user_data.get("nickname", user_data.get("email", "").split("@")[0]),
                "created_at": user_data.get("createdAt", datetime.datetime.now()),
                "last_active": datetime.datetime.now(),
                "settings": {
                    "new_cards_per_day": user_data.get("settings", {}).get("new_cards_per_day", 10),
                    "can_access_gakushi": gakushi_permission,
                    "notifications_enabled": user_data.get("settings", {}).get("notifications_enabled", True),
                    "theme": user_data.get("settings", {}).get("theme", "light")
                },
                "stats": stats,
                "migration": {
                    "migrated_at": datetime.datetime.now(),
                    "original_cards_count": len(old_cards_docs),
                    "migrated_cards_count": migrated_cards,
                    "migration_version": "v2.0"
                }
            }
            
            # 6. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self.db.collection("users").document(uid).set(optimized_user_data)
            
            # 7. ç§»è¡Œãƒ­ã‚°ã«è¨˜éŒ²
            self.migration_log.append({
                "uid": uid,
                "migrated_at": datetime.datetime.now(),
                "cards_migrated": migrated_cards,
                "status": "success"
            })
            
            logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ç§»è¡Œå®Œäº†: ã‚«ãƒ¼ãƒ‰{migrated_cards}æš")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {uid[:8]} ç§»è¡Œå¤±æ•—: {e}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«è¨˜éŒ²
            self.migration_log.append({
                "uid": uid,
                "migrated_at": datetime.datetime.now(),
                "error": str(e),
                "status": "failed"
            })
            
            return False
    
    def _convert_to_optimized_card(self, uid: str, question_id: str, old_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ—§ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–æ§‹é€ ã«å¤‰æ›"""
        history = old_data.get("history", [])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—
        total_attempts = len(history)
        correct_attempts = sum(1 for h in history if h.get("quality", 0) >= 4)
        avg_quality = sum(h.get("quality", 0) for h in history) / max(total_attempts, 1)
        last_quality = history[-1].get("quality", 0) if history else 0
        
        return {
            "uid": uid,
            "question_id": question_id,
            "sm2_data": {
                "n": old_data.get("n", 0),
                "ef": old_data.get("ef", 2.5),
                "interval": old_data.get("interval", 0),
                "due_date": old_data.get("dueDate", datetime.datetime.now()),
                "last_studied": old_data.get("lastReviewed")
            },
            "performance": {
                "total_attempts": total_attempts,
                "correct_attempts": correct_attempts,
                "avg_quality": avg_quality,
                "last_quality": last_quality
            },
            "metadata": {
                "created_at": old_data.get("createdAt", datetime.datetime.now()),
                "updated_at": datetime.datetime.now(),
                "subject": self.optimizer._get_subject_from_question_id(question_id),
                "difficulty": old_data.get("difficulty", "normal"),
                "original_level": old_data.get("level", 0)  # å‚è€ƒç”¨
            },
            "history": history[-10:]  # æœ€æ–°10ä»¶ã®ã¿ä¿æŒ
        }
    
    def migrate_all_users_completely(self) -> bool:
        """å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Œå…¨ç§»è¡Œ"""
        try:
            logger.info("=== å®Œå…¨ç§»è¡Œãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ ===")
            
            # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_id = self.backup_existing_data()
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_id}")
            
            # 2. å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—
            users_ref = self.db.collection("users")
            users_docs = list(users_ref.stream())
            
            valid_users = []
            for user_doc in users_docs:
                user_data = user_doc.to_dict()
                if user_data.get("email"):
                    valid_users.append(user_doc.id)
            
            logger.info(f"ç§»è¡Œå¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: {len(valid_users)}å")
            
            # 3. æ®µéšçš„ç§»è¡Œå®Ÿè¡Œ
            success_count = 0
            failure_count = 0
            
            for i, uid in enumerate(valid_users, 1):
                logger.info(f"é€²æ—: {i}/{len(valid_users)} - {uid[:8]}")
                
                if self.migrate_user_data_safe(uid):
                    success_count += 1
                else:
                    failure_count += 1
                
                # é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ
                if i % 10 == 0:
                    logger.info(f"ä¸­é–“å ±å‘Š: æˆåŠŸ{success_count}, å¤±æ•—{failure_count}")
                
                # è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
                time.sleep(0.1)
            
            # 4. ç§»è¡Œå¾Œå‡¦ç†
            logger.info("ç§»è¡Œå¾Œå‡¦ç†é–‹å§‹")
            
            # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆã®å†è¨ˆç®—
            self.ranking_system.update_all_user_statistics()
            
            # é€±é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜
            self.ranking_system.save_weekly_ranking_snapshot()
            
            # 5. ç§»è¡Œãƒ­ã‚°ã®ä¿å­˜
            migration_summary = {
                "migration_id": f"complete_migration_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "started_at": datetime.datetime.now(),
                "backup_id": backup_id,
                "total_users": len(valid_users),
                "success_count": success_count,
                "failure_count": failure_count,
                "migration_log": self.migration_log,
                "status": "completed"
            }
            
            summary_ref = self.db.collection("migration_summaries").document(migration_summary["migration_id"])
            summary_ref.set(migration_summary)
            
            logger.info("=== å®Œå…¨ç§»è¡Œãƒ—ãƒ­ã‚»ã‚¹å®Œäº† ===")
            logger.info(f"çµæœ: æˆåŠŸ{success_count}å, å¤±æ•—{failure_count}å")
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ID: {backup_id}")
            
            return failure_count == 0
            
        except Exception as e:
            logger.error(f"å®Œå…¨ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def validate_migration_results(self) -> Dict[str, Any]:
        """ç§»è¡Œçµæœã®æ¤œè¨¼"""
        try:
            logger.info("ç§»è¡Œçµæœæ¤œè¨¼é–‹å§‹")
            
            validation_results = {
                "validation_time": datetime.datetime.now(),
                "users": {"total": 0, "migrated": 0, "issues": []},
                "study_cards": {"total": 0, "valid": 0, "issues": []},
                "statistics": {"accurate": 0, "inaccurate": 0, "issues": []},
                "overall_status": "unknown"
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            users_ref = self.db.collection("users")
            users_docs = list(users_ref.stream())
            
            for user_doc in users_docs:
                user_data = user_doc.to_dict()
                validation_results["users"]["total"] += 1
                
                if user_data.get("migration", {}).get("migration_version") == "v2.0":
                    validation_results["users"]["migrated"] += 1
                else:
                    validation_results["users"]["issues"].append(f"User {user_doc.id[:8]} not migrated")
            
            # study_cardsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ¤œè¨¼
            study_cards_ref = self.db.collection("study_cards")
            study_cards_docs = list(study_cards_ref.stream())
            
            for card_doc in study_cards_docs:
                card_data = card_doc.to_dict()
                validation_results["study_cards"]["total"] += 1
                
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
                required_fields = ["uid", "question_id", "sm2_data", "performance", "metadata"]
                if all(field in card_data for field in required_fields):
                    validation_results["study_cards"]["valid"] += 1
                else:
                    validation_results["study_cards"]["issues"].append(f"Card {card_doc.id} missing required fields")
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            for user_doc in users_docs:
                user_data = user_doc.to_dict()
                stats = user_data.get("stats", {})
                
                if stats.get("last_updated"):
                    validation_results["statistics"]["accurate"] += 1
                else:
                    validation_results["statistics"]["inaccurate"] += 1
                    validation_results["statistics"]["issues"].append(f"User {user_doc.id[:8]} missing stats")
            
            # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if (validation_results["users"]["migrated"] == validation_results["users"]["total"] and
                validation_results["study_cards"]["valid"] == validation_results["study_cards"]["total"] and
                validation_results["statistics"]["inaccurate"] == 0):
                validation_results["overall_status"] = "success"
            else:
                validation_results["overall_status"] = "issues_found"
            
            logger.info(f"ç§»è¡Œçµæœæ¤œè¨¼å®Œäº†: {validation_results['overall_status']}")
            return validation_results
            
        except Exception as e:
            logger.error(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {"overall_status": "error", "error": str(e)}
    
    def get_deletable_collections_info(self) -> Dict[str, Any]:
        """ç§»è¡Œå¾Œã«å‰Šé™¤å¯èƒ½ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±"""
        return {
            "deletable_collections": [
                {
                    "name": "user_permissions",
                    "reason": "å­¦å£«æ¨©é™ãŒusers.settings.can_access_gakushiã«ç§»è¡Œæ¸ˆã¿",
                    "risk_level": "ä½",
                    "estimated_documents": "å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã¨åŒç¨‹åº¦"
                },
                {
                    "name": "userCards (ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³)",
                    "reason": "å…¨ã¦study_cardsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«æœ€é©åŒ–ç§»è¡Œæ¸ˆã¿",
                    "risk_level": "ä½",
                    "estimated_documents": "å…¨å­¦ç¿’ã‚«ãƒ¼ãƒ‰æ•°"
                },
                {
                    "name": "user_progress",
                    "reason": "çµ±è¨ˆæƒ…å ±ãŒusers.statsã«çµ±åˆæ¸ˆã¿",
                    "risk_level": "ä½",
                    "estimated_documents": "å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã¨åŒç¨‹åº¦"
                },
                {
                    "name": "user_profiles",
                    "reason": "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ãŒusersã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«çµ±åˆæ¸ˆã¿",
                    "risk_level": "ä½",
                    "estimated_documents": "å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã¨åŒç¨‹åº¦"
                },
                {
                    "name": "learningLogs",
                    "reason": "å­¦ç¿’å±¥æ­´ãŒæœ€é©åŒ–ã•ã‚Œã¦study_cardsã«çµ±åˆæ¸ˆã¿",
                    "risk_level": "ä¸­",
                    "estimated_documents": "æ•°ä¸‡ã€œæ•°åä¸‡ä»¶"
                }
            ],
            "keep_collections": [
                {
                    "name": "users",
                    "reason": "æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿"
                },
                {
                    "name": "study_cards",
                    "reason": "æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿"
                },
                {
                    "name": "weekly_rankings",
                    "reason": "æœ€é©åŒ–ã•ã‚ŒãŸãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿"
                },
                {
                    "name": "migration_backups",
                    "reason": "ç§»è¡Œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"
                },
                {
                    "name": "analytics_events",
                    "reason": "åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰Šé™¤ä¸è¦ï¼‰"
                }
            ],
            "deletion_command": """
# å‰Šé™¤ã‚³ãƒãƒ³ãƒ‰ä¾‹ï¼ˆæ…é‡ã«å®Ÿè¡Œï¼‰
# 1. user_permissionså‰Šé™¤
# 2. å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®userCardsã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤
# 3. user_progress, user_profiles, learningLogså‰Šé™¤
            """,
            "warning": "å‰Šé™¤å‰ã«å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å­˜åœ¨ç¢ºèªã¨å‹•ä½œãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„"
        }


# === ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° ===

def run_complete_migration():
    """å®Œå…¨ç§»è¡Œã®å®Ÿè¡Œ"""
    migration_system = CompleteMigrationSystem()
    
    print("ğŸš€ Firestoreå®Œå…¨æœ€é©åŒ–ç§»è¡Œã‚’é–‹å§‹ã—ã¾ã™")
    print("âš ï¸  ã“ã®å‡¦ç†ã¯æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ã¾ã™ã€‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè‡ªå‹•ä½œæˆã•ã‚Œã¾ã™ã€‚")
    
    # ç§»è¡Œå®Ÿè¡Œ
    success = migration_system.migrate_all_users_completely()
    
    if success:
        print("âœ… ç§»è¡ŒãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        
        # çµæœæ¤œè¨¼
        validation = migration_system.validate_migration_results()
        
        if validation["overall_status"] == "success":
            print("âœ… ç§»è¡Œçµæœã®æ¤œè¨¼ã‚‚æˆåŠŸã—ã¾ã—ãŸ")
            
            # å‰Šé™¤å¯èƒ½ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º
            deletion_info = migration_system.get_deletable_collections_info()
            print("\nğŸ“‹ ç§»è¡Œå¾Œã®å‰Šé™¤æ¨å¥¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³:")
            for collection in deletion_info["deletable_collections"]:
                print(f"  - {collection['name']}: {collection['reason']}")
            
            print(f"\nâš ï¸  {deletion_info['warning']}")
        else:
            print(f"âš ï¸  æ¤œè¨¼ã§å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {validation}")
    else:
        print("âŒ ç§»è¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    return success


if __name__ == "__main__":
    run_complete_migration()
