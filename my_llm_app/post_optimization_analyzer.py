#!/usr/bin/env python3
"""
æœ€é©åŒ–å¾ŒFirestoreæ§‹é€ ã®è©³ç´°åˆ†æ
å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å¿…è¦æ€§ã€é‡è¤‡ã€ã•ã‚‰ãªã‚‹çµ±åˆå¯èƒ½æ€§ã‚’å¾¹åº•æ¤œè¨¼
"""

from complete_migration_system import CompleteMigrationSystem
from datetime import datetime
import json

class PostOptimizationAnalyzer:
    def __init__(self):
        self.migration_system = CompleteMigrationSystem()
        self.db = self.migration_system.db
        
    def analyze_current_structure(self):
        """ç¾åœ¨ã®æ§‹é€ ã‚’è©³ç´°åˆ†æ"""
        print("ğŸ” æœ€é©åŒ–å¾Œæ§‹é€ ã®è©³ç´°åˆ†æ")
        print("=" * 60)
        
        analysis_result = {
            'current_collections': {},
            'redundancy_analysis': {},
            'consolidation_opportunities': [],
            'final_recommendations': []
        }
        
        # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
        collections = [
            'analytics_summary',
            'daily_active_users', 
            'daily_engagement_summary',
            'migration_backups',
            'migration_summaries', 
            'study_cards',
            'system_stats',
            'user_rankings',
            'users',
            'weekly_ranking_snapshots',
            'weekly_rankings'
        ]
        
        for collection_name in collections:
            print(f"\nğŸ“Š {collection_name} åˆ†æ")
            analysis = self._analyze_collection_deep(collection_name)
            analysis_result['current_collections'][collection_name] = analysis
            self._print_collection_analysis(collection_name, analysis)
        
        # æ©Ÿèƒ½çš„é‡è¤‡åˆ†æ
        redundancy = self._analyze_functional_redundancy()
        analysis_result['redundancy_analysis'] = redundancy
        
        # çµ±åˆæ©Ÿä¼šã®ç‰¹å®š
        consolidation = self._identify_consolidation_opportunities()
        analysis_result['consolidation_opportunities'] = consolidation
        
        # æœ€çµ‚æ¨å¥¨
        recommendations = self._generate_post_optimization_recommendations()
        analysis_result['final_recommendations'] = recommendations
        
        self._print_optimization_summary(analysis_result)
        
        return analysis_result
    
    def _analyze_collection_deep(self, collection_name):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æ·±åº¦åˆ†æ"""
        try:
            collection = self.db.collection(collection_name)
            docs = list(collection.stream())
            
            analysis = {
                'document_count': len(docs),
                'unique_users': set(),
                'data_patterns': {},
                'size_analysis': {},
                'functional_purpose': '',
                'redundancy_score': 0,
                'optimization_potential': 0,
                'necessity_score': 0
            }
            
            total_data_size = 0
            
            for doc in docs:
                data = doc.to_dict()
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å®š
                for user_field in ['uid', 'userId', 'user_id']:
                    if user_field in data:
                        analysis['unique_users'].add(data[user_field])
                        break
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
                doc_size = len(str(data))
                total_data_size += doc_size
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                for key in data.keys():
                    analysis['data_patterns'][key] = analysis['data_patterns'].get(key, 0) + 1
            
            analysis['unique_users'] = len(analysis['unique_users'])
            analysis['size_analysis'] = {
                'total_size': total_data_size,
                'avg_doc_size': total_data_size // len(docs) if docs else 0
            }
            
            # æ©Ÿèƒ½çš„ç›®çš„ã¨å¿…è¦æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            analysis.update(self._score_collection_necessity(collection_name, docs))
            
            return analysis
            
        except Exception as e:
            return {'error': str(e)}
    
    def _score_collection_necessity(self, collection_name, docs):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å¿…è¦æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        scoring = {
            'functional_purpose': '',
            'necessity_score': 0,
            'redundancy_score': 0, 
            'optimization_potential': 0
        }
        
        if collection_name == 'users':
            scoring.update({
                'functional_purpose': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒã‚¹ã‚¿ãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿',
                'necessity_score': 100,
                'redundancy_score': 0,
                'optimization_potential': 10
            })
        
        elif collection_name == 'study_cards':
            scoring.update({
                'functional_purpose': 'å­¦ç¿’ã‚«ãƒ¼ãƒ‰ãƒ»é€²æ—ç®¡ç†',
                'necessity_score': 100,
                'redundancy_score': 0,
                'optimization_potential': 5
            })
        
        elif collection_name == 'weekly_rankings':
            scoring.update({
                'functional_purpose': 'é€±æ¬¡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¡ã‚¤ãƒ³',
                'necessity_score': 95,
                'redundancy_score': 5,
                'optimization_potential': 15
            })
        
        elif collection_name == 'user_rankings':
            scoring.update({
                'functional_purpose': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°è©³ç´°',
                'necessity_score': 80,
                'redundancy_score': 30,
                'optimization_potential': 40
            })
        
        elif collection_name == 'weekly_ranking_snapshots':
            scoring.update({
                'functional_purpose': 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ',
                'necessity_score': 60,
                'redundancy_score': 60,
                'optimization_potential': 70
            })
        
        elif collection_name == 'analytics_summary':
            scoring.update({
                'functional_purpose': 'æ—¥æ¬¡åˆ†æã‚µãƒãƒªãƒ¼',
                'necessity_score': 50,
                'redundancy_score': 70,
                'optimization_potential': 80
            })
        
        elif collection_name == 'daily_active_users':
            scoring.update({
                'functional_purpose': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£è¿½è·¡',
                'necessity_score': 45,
                'redundancy_score': 75,
                'optimization_potential': 85
            })
        
        elif collection_name == 'daily_engagement_summary':
            scoring.update({
                'functional_purpose': 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé›†è¨ˆ',
                'necessity_score': 40,
                'redundancy_score': 80,
                'optimization_potential': 90
            })
        
        elif collection_name == 'system_stats':
            scoring.update({
                'functional_purpose': 'ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ',
                'necessity_score': 70,
                'redundancy_score': 20,
                'optimization_potential': 30
            })
        
        elif collection_name == 'migration_backups':
            scoring.update({
                'functional_purpose': 'ç§»è¡Œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—',
                'necessity_score': 30,
                'redundancy_score': 90,
                'optimization_potential': 95
            })
        
        elif collection_name == 'migration_summaries':
            scoring.update({
                'functional_purpose': 'ç§»è¡Œã‚µãƒãƒªãƒ¼',
                'necessity_score': 25,
                'redundancy_score': 95,
                'optimization_potential': 95
            })
        
        return scoring
    
    def _analyze_functional_redundancy(self):
        """æ©Ÿèƒ½çš„é‡è¤‡ã®åˆ†æ"""
        print("\nğŸ”„ æ©Ÿèƒ½çš„é‡è¤‡åˆ†æ")
        
        redundancy_groups = {
            'ranking_related': {
                'collections': ['weekly_rankings', 'user_rankings', 'weekly_ranking_snapshots'],
                'overlap_type': 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ©Ÿèƒ½é‡è¤‡',
                'consolidation_potential': 85
            },
            'analytics_related': {
                'collections': ['analytics_summary', 'daily_active_users', 'daily_engagement_summary'],
                'overlap_type': 'åˆ†ææ©Ÿèƒ½é‡è¤‡',
                'consolidation_potential': 90
            },
            'migration_related': {
                'collections': ['migration_backups', 'migration_summaries'],
                'overlap_type': 'ç§»è¡Œé–¢é€£ï¼ˆä¸€æ™‚çš„ï¼‰',
                'consolidation_potential': 100
            }
        }
        
        for group_name, group_info in redundancy_groups.items():
            print(f"   {group_name}: {group_info['overlap_type']} - çµ±åˆå¯èƒ½æ€§ {group_info['consolidation_potential']}%")
        
        return redundancy_groups
    
    def _identify_consolidation_opportunities(self):
        """çµ±åˆæ©Ÿä¼šã®ç‰¹å®š"""
        print("\nâš¡ çµ±åˆæ©Ÿä¼š")
        
        opportunities = [
            {
                'priority': 1,
                'action': 'delete_migration_data',
                'target': ['migration_backups', 'migration_summaries'],
                'reason': 'ç§»è¡Œå®Œäº†ã«ã‚ˆã‚Šä¸è¦ã€100%å‰Šé™¤å¯èƒ½',
                'impact': 'minimal',
                'risk': 'none'
            },
            {
                'priority': 2,
                'action': 'consolidate_analytics',
                'target': ['daily_active_users', 'daily_engagement_summary', 'analytics_summary'],
                'reason': 'æ©Ÿèƒ½é‡è¤‡90%ã€å˜ä¸€analytics_dailyã«çµ±åˆå¯èƒ½',
                'impact': 'medium',
                'risk': 'low'
            },
            {
                'priority': 3,
                'action': 'optimize_rankings',
                'target': ['user_rankings', 'weekly_ranking_snapshots'],
                'reason': 'weekly_rankingsã«çµ±åˆã€å†—é•·æ€§å‰Šé™¤',
                'impact': 'medium',
                'risk': 'low'
            },
            {
                'priority': 4,
                'action': 'integrate_system_stats',
                'target': ['system_stats'],
                'reason': 'usersã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«çµ±åˆå¯èƒ½',
                'impact': 'small',
                'risk': 'minimal'
            }
        ]
        
        for opp in opportunities:
            print(f"   {opp['priority']}. {opp['action']}: {opp['target']}")
            print(f"      ç†ç”±: {opp['reason']}")
            print(f"      å½±éŸ¿: {opp['impact']}, ãƒªã‚¹ã‚¯: {opp['risk']}")
        
        return opportunities
    
    def _generate_post_optimization_recommendations(self):
        """æœ€é©åŒ–å¾Œã®æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            'immediate_actions': [
                {
                    'action': 'delete_migration_collections',
                    'collections': ['migration_backups', 'migration_summaries'],
                    'justification': 'ç§»è¡Œå®Œäº†ã«ã‚ˆã‚Šç›®çš„é”æˆã€ä¿æŒä¸è¦',
                    'doc_reduction': 2,
                    'collection_reduction': 2
                }
            ],
            'consolidation_actions': [
                {
                    'action': 'merge_analytics_collections',
                    'source_collections': ['analytics_summary', 'daily_active_users', 'daily_engagement_summary'],
                    'target_collection': 'analytics_daily',
                    'justification': 'æ©Ÿèƒ½é‡è¤‡è§£æ¶ˆã€ç®¡ç†ç°¡ç´ åŒ–',
                    'doc_reduction': 15,
                    'collection_reduction': 2
                },
                {
                    'action': 'optimize_ranking_structure',
                    'source_collections': ['user_rankings', 'weekly_ranking_snapshots'],
                    'target_integration': 'weekly_rankings',
                    'justification': 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿çµ±åˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š',
                    'doc_reduction': 31,
                    'collection_reduction': 2
                }
            ],
            'final_structure': [
                'users (33 docs)',
                'study_cards (1000 docs)', 
                'weekly_rankings (çµ±åˆå¾Œ)',
                'analytics_daily (çµ±åˆå¾Œ)',
                'system_stats (1 doc)'
            ],
            'target_metrics': {
                'final_collections': 5,
                'final_documents': '~1036',
                'additional_reduction': '4.4%',
                'management_complexity': 'minimal'
            }
        }
        
        return recommendations
    
    def _print_collection_analysis(self, collection_name, analysis):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åˆ†æçµæœè¡¨ç¤º"""
        if 'error' in analysis:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {analysis['error']}")
            return
            
        print(f"   ğŸ“ˆ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {analysis['document_count']}")
        print(f"   ğŸ‘¥ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼: {analysis['unique_users']}")
        print(f"   ğŸ¯ æ©Ÿèƒ½: {analysis['functional_purpose']}")
        print(f"   ğŸ’¯ å¿…è¦æ€§ã‚¹ã‚³ã‚¢: {analysis['necessity_score']}/100")
        print(f"   ğŸ”„ é‡è¤‡ã‚¹ã‚³ã‚¢: {analysis['redundancy_score']}/100") 
        print(f"   âš¡ æœ€é©åŒ–å¯èƒ½æ€§: {analysis['optimization_potential']}/100")
        
        if analysis['size_analysis']['avg_doc_size'] > 0:
            print(f"   ğŸ“Š å¹³å‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚º: {analysis['size_analysis']['avg_doc_size']}æ–‡å­—")
    
    def _print_optimization_summary(self, analysis_result):
        """æœ€é©åŒ–ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ¯ è¿½åŠ æœ€é©åŒ–å¯èƒ½æ€§ã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        
        high_optimization = []
        medium_optimization = []
        low_optimization = []
        
        for collection_name, analysis in analysis_result['current_collections'].items():
            if 'optimization_potential' in analysis:
                potential = analysis['optimization_potential']
                if potential >= 80:
                    high_optimization.append((collection_name, potential))
                elif potential >= 50:
                    medium_optimization.append((collection_name, potential))
                else:
                    low_optimization.append((collection_name, potential))
        
        print("\nğŸ”¥ é«˜æœ€é©åŒ–å¯èƒ½æ€§ (80%+):")
        for name, score in high_optimization:
            print(f"   {name}: {score}%")
        
        print("\nâš¡ ä¸­æœ€é©åŒ–å¯èƒ½æ€§ (50-79%):")
        for name, score in medium_optimization:
            print(f"   {name}: {score}%")
        
        print("\nâœ… æœ€é©åŒ–æ¸ˆã¿ (<50%):")
        for name, score in low_optimization:
            print(f"   {name}: {score}%")
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        recommendations = analysis_result['final_recommendations']
        
        print("\nğŸ“‹ è¿½åŠ æœ€é©åŒ–æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("1. ç§»è¡Œé–¢é€£å‰Šé™¤ â†’ -2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
        print("2. åˆ†æç³»çµ±åˆ â†’ -2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")  
        print("3. ãƒ©ãƒ³ã‚­ãƒ³ã‚°æœ€é©åŒ– â†’ -2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
        print("4. ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆçµ±åˆ â†’ -1ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
        
        print("\nğŸ¯ æœ€çµ‚ç›®æ¨™æ§‹é€ :")
        for structure in recommendations['target_metrics']:
            print(f"   {structure}: {recommendations['target_metrics'][structure]}")

if __name__ == "__main__":
    analyzer = PostOptimizationAnalyzer()
    analysis_result = analyzer.analyze_current_structure()
    
    print("\nğŸ“Š è©³ç´°åˆ†æå®Œäº†")
    print("ã•ã‚‰ãªã‚‹æœ€é©åŒ–æ©Ÿä¼šãŒç‰¹å®šã•ã‚Œã¾ã—ãŸ")
