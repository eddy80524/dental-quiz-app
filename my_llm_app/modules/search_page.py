"""
æ¤œç´¢ãƒ»é€²æ—ãƒšãƒ¼ã‚¸ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - è»½é‡åŒ–ç‰ˆ

AI Copilotå‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¦ä»¶ã‚’å®Œå…¨ã«æº€ãŸã™çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
- çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: å­¦ç¿’çŠ¶æ³ã‚µãƒãƒªãƒ¼ï¼ˆå­¦ç¿’æ¸ˆã¿å•é¡Œæ•°ã€ç¿’å¾—ç‡ã€ç·å­¦ç¿’å›æ•°ã€è¨˜æ†¶å®šç€åº¦ï¼‰
- ã‚¿ãƒ–ãƒ™ãƒ¼ã‚¹UI: æ¦‚è¦ã€ã‚°ãƒ©ãƒ•åˆ†æã€å•é¡Œãƒªã‚¹ãƒˆã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®4ã¤ã®ã‚¿ãƒ–
- ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨é€£å‹•ã—ãŸå‹•çš„çµã‚Šè¾¼ã¿
- è©³ç´°ãªé€²æ—åˆ†æ: ç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒã€æ­£è§£ç‡ã€ç§‘ç›®åˆ¥åˆ†æã€æ—¥ã€…ã®å­¦ç¿’é‡å¯è¦–åŒ–
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: å•é¡Œæ–‡ãƒ»ç§‘ç›®ãƒ»å•é¡Œç•ªå·æ¤œç´¢ã€PDFç”Ÿæˆæ©Ÿèƒ½
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã€é…å»¶èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†æœ€é©åŒ–
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import datetime
from typing import Dict, List, Any, Optional
import time
import base64
from functools import lru_cache
import hashlib
import json
import re
import random
import sys
import os
import subprocess
import shutil
import tempfile
import hashlib
from collections import defaultdict, Counter
from functools import lru_cache

# å¿…è¦ãªãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import (
    ALL_QUESTIONS, 
    HISSHU_Q_NUMBERS_SET, 
    GAKUSHI_HISSHU_Q_NUMBERS_SET,
    extract_year_from_question_number,
    export_questions_to_latex_tcb_jsarticle,
    _gather_images_for_questions,
    _image_block_latex,
    compile_latex_to_pdf
)
from firestore_db import get_firestore_manager

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append('/Users/utsueito/kokushi-dx-poc/dental-DX-PoC')
try:
    from user_data_extractor import UserDataExtractor
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šUserDataExtractorãŒåˆ©ç”¨ã§ããªã„å ´åˆ
    UserDataExtractor = None

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹
class SearchPageCache:
    """æ¤œç´¢ãƒ»é€²æ—ãƒšãƒ¼ã‚¸ç”¨ã®è»½é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    _instance = None
    _data_cache = {}
    _cache_timestamps = {}
    CACHE_TIMEOUT = 300  # 5åˆ†ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™åˆ‡ã‚Œ
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def get_cached_data(cls, cache_key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        current_time = time.time()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹æœŸé™å†…ã®å ´åˆ
        if (cache_key in cls._data_cache and 
            cache_key in cls._cache_timestamps and
            current_time - cls._cache_timestamps[cache_key] < cls.CACHE_TIMEOUT):
            return cls._data_cache[cache_key]
        
        return None
    
    @classmethod
    def set_cached_data(cls, cache_key: str, data):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cls._data_cache[cache_key] = data
        cls._cache_timestamps[cache_key] = time.time()
    
    @classmethod
    def clear_cache(cls):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        cls._data_cache.clear()
        cls._cache_timestamps.clear()

@lru_cache(maxsize=100)
def get_cached_card_level(card_data_hash: str, n: int, ef: float) -> str:
    """ã‚«ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨ˆç®—ã®çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    return _calculate_card_level_internal(n, ef)

def _calculate_card_level_internal(n: int, ef: float) -> str:
    """å†…éƒ¨çš„ãªã‚«ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨ˆç®—é–¢æ•°"""
    # SM2ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ãç¿’ç†Ÿåº¦è¨ˆç®—
    if (ef >= 2.8 and n >= 3) or (ef >= 2.5 and n >= 5) or (n >= 8):
        return "ç¿’å¾—æ¸ˆã¿"
    if n >= 7: return "ãƒ¬ãƒ™ãƒ«5"
    if n >= 6: return "ãƒ¬ãƒ™ãƒ«4"
    if n >= 4: return "ãƒ¬ãƒ™ãƒ«3"
    if n >= 3: return "ãƒ¬ãƒ™ãƒ«2"
    if n >= 2: return "ãƒ¬ãƒ™ãƒ«1"
    return "ãƒ¬ãƒ™ãƒ«0"

# çµ±ä¸€ã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«è‰²åˆ†ã‘å®šç¾©ï¼ˆæ–°ãƒ‡ã‚¶ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
LEVEL_COLORS = {
    "æœªå­¦ç¿’": "#BDBDBD",
    "ãƒ¬ãƒ™ãƒ«0": "#E47C2E",  # ãƒ¬ãƒ™ãƒ«0ã‚’å†å°å…¥ï¼ˆæ·¡ã„èµ¤è‰²ã§å­¦ç¿’é–‹å§‹æ®µéšã‚’ç¤ºã™ï¼‰
    "ãƒ¬ãƒ™ãƒ«1": "#F4B83E", 
    "ãƒ¬ãƒ™ãƒ«2": "#56C68B", 
    "ãƒ¬ãƒ™ãƒ«3": "#B06CCF",
    "ãƒ¬ãƒ™ãƒ«4": "#4AB2D9",
    "ãƒ¬ãƒ™ãƒ«5": "#7C5FCF", 
    "ç¿’å¾—æ¸ˆã¿": "#344A90"
}

# çµ±ä¸€ã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«é †åºå®šç¾©ï¼ˆ0-5ãƒ¬ãƒ™ãƒ«ã‚·ã‚¹ãƒ†ãƒ ï¼‰
LEVEL_ORDER = ["æœªå­¦ç¿’", "ãƒ¬ãƒ™ãƒ«0", "ãƒ¬ãƒ™ãƒ«1", "ãƒ¬ãƒ™ãƒ«2", "ãƒ¬ãƒ™ãƒ«3", "ãƒ¬ãƒ™ãƒ«4", "ãƒ¬ãƒ™ãƒ«5", "ç¿’å¾—æ¸ˆã¿"]

def check_gakushi_permission(uid: str) -> bool:
    """å­¦å£«è©¦é¨“ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        db = get_firestore_manager()
        user_ref = db.collection('users').document(uid)
        user_doc = user_ref.get()
        
        if user_doc.exists:
            user_data = user_doc.to_dict()
            return user_data.get('has_gakushi_permission', False)
        
        # æ¨©é™æƒ…å ±ãŒãªã„å ´åˆã¯Trueã‚’è¿”ã™ï¼ˆé–‹ç™ºæ™‚ã®ä¾¿å®œï¼‰
        return True
    except Exception:
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚Trueã‚’è¿”ã™ï¼ˆé–‹ç™ºæ™‚ã®ä¾¿å®œï¼‰
        return True

def generate_test_cards_data(num_cards: int = 100) -> Dict[str, Any]:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    æ¼”ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã„å ´åˆã§ã‚‚ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚’ãƒ†ã‚¹ãƒˆã§ãã‚‹
    å®Ÿéš›ã®å•é¡ŒIDã«å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    """
    import random
    from datetime import datetime, timedelta
    
    test_cards = {}
    levels = ["æœªå­¦ç¿’", "ãƒ¬ãƒ™ãƒ«0", "ãƒ¬ãƒ™ãƒ«1", "ãƒ¬ãƒ™ãƒ«2", "ãƒ¬ãƒ™ãƒ«3", "ç¿’å¾—æ¸ˆã¿"]
    level_weights = [0.4, 0.2, 0.15, 0.1, 0.1, 0.05]  # æœªå­¦ç¿’ãŒå¤šã‚
    
    # å®Ÿéš›ã®å•é¡ŒIDã‚’ä½¿ç”¨ï¼ˆæœ€åˆã®num_cardsä»¶ï¼‰
    # ALL_QUESTIONSã¯æ—¢ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã®ã§ä½¿ç”¨
    actual_questions = list(ALL_QUESTIONS)[:num_cards] if len(ALL_QUESTIONS) >= num_cards else ALL_QUESTIONS
    
    for question in actual_questions:
        q_id = question.get('number', f"test_{len(test_cards):04d}")
        level = random.choices(levels, weights=level_weights)[0]
        
        # ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸå±¥æ­´ã‚’ç”Ÿæˆ
        history = []
        if level != "æœªå­¦ç¿’":
            num_history = random.randint(1, 10)
            for j in range(num_history):
                history.append({
                    'timestamp': datetime.now() - timedelta(days=random.randint(1, 30)),
                    'quality': random.randint(0, 5),
                    'is_correct': random.choice([True, False]),
                    'user_answer': random.randint(1, 4),
                    'time_spent': random.randint(10, 120)
                })
        
        test_cards[q_id] = {
            'n': random.randint(0, 20) if level != "æœªå­¦ç¿’" else 0,
            'EF': round(random.uniform(1.3, 3.0), 2) if level != "æœªå­¦ç¿’" else 2.5,
            'history': history
        }
    
    return test_cards

def calculate_card_level(card: Dict[str, Any]) -> str:
    """
    ã‚«ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨ˆç®—é–¢æ•°ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰
    
    å®Ÿéš›ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã«ã¯'level'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨'mastery_status'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€
    ã“ã‚Œã‚‰ã‚’é©åˆ‡ã«å¤‰æ›ã—ã¦ãƒ¬ãƒ™ãƒ«æ–‡å­—åˆ—ã‚’è¿”ã™
    """
    # 1. ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€Œæœªå­¦ç¿’ã€
    if not card or not isinstance(card, dict):
        return "æœªå­¦ç¿’"
    
    # 2. mastery_statusãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
    mastery_status = card.get('mastery_status')
    if mastery_status:
        # mastery_statusã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆã€Œç¿’å¾—æ¸ˆã¿ã€ãªã©ï¼‰
        return mastery_status
    
    # 3. levelãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒ¬ãƒ™ãƒ«ç•ªå·ã‹ã‚‰æ–‡å­—åˆ—ã«å¤‰æ›
    level = card.get('level')
    if level is not None:
        if level == 0:
            return "ãƒ¬ãƒ™ãƒ«0"
        elif level == 1:
            return "ãƒ¬ãƒ™ãƒ«1"
        elif level == 2:
            return "ãƒ¬ãƒ™ãƒ«2"
        elif level == 3:
            return "ãƒ¬ãƒ™ãƒ«3"
        elif level == 4:
            return "ãƒ¬ãƒ™ãƒ«4"
        elif level == 5:
            return "ãƒ¬ãƒ™ãƒ«5"
        elif level >= 6:
            return "ç¿’å¾—æ¸ˆã¿"
    
    # 4. å­¦ç¿’å±¥æ­´ãŒã‚ã‚‹ã‹ã©ã†ã‹ã§åˆ¤å®š
    history_count = card.get('history_count', 0)
    total_attempts = card.get('total_attempts', 0)
    
    if history_count > 0 or total_attempts > 0:
        # å­¦ç¿’å±¥æ­´ãŒã‚ã‚‹ãŒå…·ä½“çš„ãªãƒ¬ãƒ™ãƒ«ãŒä¸æ˜ãªå ´åˆ
        return "ãƒ¬ãƒ™ãƒ«0"
    
    # 5. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœªå­¦ç¿’
    return "æœªå­¦ç¿’"
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œç‰ˆã®ã‚«ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«è¨ˆç®—é–¢æ•°ï¼š
    - ã€Œæœªå­¦ç¿’ã€ã¯å±¥æ­´ã®æœ‰ç„¡ã§å³å¯†ã«åˆ¤å®š
    - ã€Œãƒ¬ãƒ™ãƒ«0ã€ã‚’é–‹å§‹ç‚¹ã¨ã™ã‚‹é€£ç¶šçš„ãªãƒ¬ãƒ™ãƒ«ã‚¢ãƒƒãƒ—
    - ã€Œç¿’å¾—æ¸ˆã¿ã€ã¯EFå€¤ã¨æ¼”ç¿’å›æ•°ã®çµ„ã¿åˆã‚ã›ã§åˆ¤å®š
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
    """
    # 1. ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯å­¦ç¿’å±¥æ­´ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€Œæœªå­¦ç¿’ã€
    if not card or not isinstance(card, dict) or not card.get('history'):
        return "æœªå­¦ç¿’"
    
    # --- ã“ã“ã‹ã‚‰å…ˆã¯ã€å­¦ç¿’å±¥æ­´ãŒ1ä»¶ä»¥ä¸Šå­˜åœ¨ã™ã‚‹å ´åˆã®å‡¦ç† ---
    
    n = card.get('n', 0)
    ef = card.get('EF', card.get('ef', 2.5))
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’ä½¿ç”¨
    card_hash = hashlib.md5(f"{n}_{ef}".encode()).hexdigest()
    return get_cached_card_level(card_hash, n, ef)

def calculate_progress_metrics(cards: Dict, base_df: pd.DataFrame, uid: str = None, analysis_target: str = "å›½è©¦å•é¡Œ") -> Dict:
    """
    å­¦ç¿’é€²æ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨å‰æ—¥æ¯”ãƒ»å‰é€±æ¯”ã‚’è¨ˆç®—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆUserDataExtractorå¼·åŒ–ç‰ˆ + ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ï¼‰
    """
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    cache_key = f"progress_metrics_{uid}_{analysis_target}_{len(cards) if cards else 0}"
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã‚’è©¦è¡Œ
    cache = SearchPageCache()
    cached_result = cache.get_cached_data(cache_key)
    if cached_result is not None:
        return cached_result
    
    today = datetime.datetime.now().date()
    yesterday = today - datetime.timedelta(days=1)
    seven_days_ago = datetime.datetime.now() - datetime.timedelta(days=7)
    fourteen_days_ago = datetime.datetime.now() - datetime.timedelta(days=14)
    
    # UserDataExtractorã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    enhanced_data = {}
    if uid and uid != "guest" and UserDataExtractor:
        try:
            extractor = UserDataExtractor()
            
            # analysis_targetã«å¿œã˜ã¦è©¦é¨“ç¨®åˆ¥ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¨­å®š
            exam_type_filter = None
            if analysis_target in ["å­¦å£«è©¦é¨“å•é¡Œ", "å­¦å£«è©¦é¨“"]:
                exam_type_filter = "å­¦å£«è©¦é¨“"
            elif analysis_target in ["å›½è©¦å•é¡Œ", "å›½è©¦"]:
                exam_type_filter = "æ­¯ç§‘å›½è©¦"
            
            # analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸãƒ­ã‚°ã‚’å–å¾—
            evaluation_logs = extractor.extract_self_evaluation_logs(uid)
            practice_data = extractor.extract_practice_logs(uid)
            
            # evaluation_logsã‚’analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if exam_type_filter and evaluation_logs:
                # å„ãƒ­ã‚°ã®å•é¡ŒIDã‹ã‚‰è©¦é¨“ç¨®åˆ¥ã‚’åˆ¤å®šã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                filtered_logs = []
                for log in evaluation_logs:
                    question_id = log.get('question_id')  # problem_id â†’ question_id ã«ä¿®æ­£
                    if question_id:
                        # å•é¡ŒIDã‹ã‚‰è©¦é¨“ç¨®åˆ¥ã‚’åˆ¤å®š
                        if exam_type_filter == "å­¦å£«è©¦é¨“" and question_id.startswith('G'):
                            # å­¦å£«è©¦é¨“å•é¡Œã®å¿…ä¿®åˆ¤å®šã‚’æ­£ã—ãè¨­å®š
                            log['is_hisshu'] = question_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                            filtered_logs.append(log)
                        elif exam_type_filter == "æ­¯ç§‘å›½è©¦" and not question_id.startswith('G'):
                            # å›½è©¦å•é¡Œã®å¿…ä¿®åˆ¤å®šã‚’æ­£ã—ãè¨­å®š
                            log['is_hisshu'] = question_id in HISSHU_Q_NUMBERS_SET
                            filtered_logs.append(log)
                        elif exam_type_filter is None:  # ãƒ•ã‚£ãƒ«ã‚¿ãªã—ã®å ´åˆ
                            # å•é¡ŒIDã«åŸºã¥ã„ã¦é©åˆ‡ãªå¿…ä¿®åˆ¤å®šã‚’è¨­å®š
                            if question_id.startswith('G'):
                                log['is_hisshu'] = question_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                            else:
                                log['is_hisshu'] = question_id in HISSHU_Q_NUMBERS_SET
                            filtered_logs.append(log)
                evaluation_logs = filtered_logs
                print(f"[INFO] {analysis_target}ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(filtered_logs)}ä»¶ (å…ƒ: ç·{len(evaluation_logs)}ä»¶)")
            else:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãªã„å ´åˆã§ã‚‚ã€ã™ã¹ã¦ã®ãƒ­ã‚°ã«æ­£ã—ã„is_hisshuãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                for log in evaluation_logs:
                    question_id = log.get('question_id')
                    if question_id:
                        if question_id.startswith('G'):
                            log['is_hisshu'] = question_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                        else:
                            log['is_hisshu'] = question_id in HISSHU_Q_NUMBERS_SET
                print(f"[INFO] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—: {len(evaluation_logs)}ä»¶")
            
            # ã‚ˆã‚Šæ­£ç¢ºãªçµ±è¨ˆã‚’è¨ˆç®—
            if evaluation_logs:
                # 7æ—¥é–“ã®æ­£è§£ç‡ã‚’æ­£ç¢ºã«è¨ˆç®—ï¼ˆanalysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰
                recent_evaluations = [
                    log for log in evaluation_logs 
                    if log['timestamp'] >= seven_days_ago
                ]
                previous_evaluations = [
                    log for log in evaluation_logs 
                    if fourteen_days_ago <= log['timestamp'] < seven_days_ago
                ]
                
                recent_correct = sum(1 for log in recent_evaluations if log.get('quality', 0) >= 3)
                previous_correct = sum(1 for log in previous_evaluations if log.get('quality', 0) >= 3)
                
                # å¿…ä¿®å•é¡Œã®æ­£è§£ç‡ã‚‚åˆ¥é€”è¨ˆç®—
                recent_hisshu_evaluations = [log for log in recent_evaluations if log.get('is_hisshu', False)]
                previous_hisshu_evaluations = [log for log in previous_evaluations if log.get('is_hisshu', False)]
                
                recent_hisshu_correct = sum(1 for log in recent_hisshu_evaluations if log.get('quality', 0) >= 3)
                previous_hisshu_correct = sum(1 for log in previous_hisshu_evaluations if log.get('quality', 0) >= 3)
                
                enhanced_data['recent_accuracy'] = (recent_correct / len(recent_evaluations) * 100) if recent_evaluations else 0
                enhanced_data['previous_accuracy'] = (previous_correct / len(previous_evaluations) * 100) if previous_evaluations else 0
                enhanced_data['recent_total'] = len(recent_evaluations)
                enhanced_data['previous_total'] = len(previous_evaluations)
                
                # å¿…ä¿®å•é¡Œã®æ­£è§£ç‡çµ±è¨ˆ
                enhanced_data['recent_hisshu_stats'] = {
                    'correct': recent_hisshu_correct,
                    'total': len(recent_hisshu_evaluations)
                }
                enhanced_data['previous_hisshu_stats'] = {
                    'correct': previous_hisshu_correct,
                    'total': len(previous_hisshu_evaluations)
                }
                
                # ä»Šæ—¥ã¨æ˜¨æ—¥ã®å­¦ç¿’æ•°ã‚’æ­£ç¢ºã«è¨ˆç®—
                today_logs = [
                    log for log in evaluation_logs 
                    if log['timestamp'].date() == today
                ]
                yesterday_logs = [
                    log for log in evaluation_logs 
                    if log['timestamp'].date() == yesterday
                ]
                
                enhanced_data['today_study_count'] = len(today_logs)
                enhanced_data['yesterday_study_count'] = len(yesterday_logs)
                
                print(f"[INFO] UserDataExtractorå¼·åŒ–({analysis_target}): ä»Šæ—¥{len(today_logs)}å•, æ˜¨æ—¥{len(yesterday_logs)}å•, ç›´è¿‘7æ—¥{len(recent_evaluations)}å•, å¿…ä¿®{len(recent_hisshu_evaluations)}å•")
        except Exception as e:
            print(f"[WARNING] UserDataExtractorå¼·åŒ–ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä»Šæ—¥ãƒ»æ˜¨æ—¥ãƒ»æœŸé–“åˆ¥ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆï¼ˆå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    today_studied_problems = set()
    yesterday_studied_problems = set()
    today_hisshu_problems = set()
    yesterday_hisshu_problems = set()
    today_study_count = enhanced_data.get('today_study_count', 0)  # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
    yesterday_study_count = enhanced_data.get('yesterday_study_count', 0)  # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
    recent_7days_stats = {'correct': 0, 'total': enhanced_data.get('recent_total', 0)}
    previous_7days_stats = {'correct': 0, 'total': enhanced_data.get('previous_total', 0)}
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ã§è£œå®Œï¼ˆUserDataExtractorãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰
    if not enhanced_data:
        for _, row in base_df.iterrows():
            q_id = row['id']
            is_hisshu = row['is_hisshu']
            card = row['card_data']
            history = card.get('history', [])
            
            if isinstance(history, list):
                for entry in history:
                    if isinstance(entry, dict):
                        timestamp = entry.get('timestamp')
                        if timestamp:
                            try:
                                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹ - DatetimeWithNanosecondså¯¾å¿œ
                                if hasattr(timestamp, 'timestamp') and callable(getattr(timestamp, 'timestamp')):
                                    # DatetimeWithNanoseconds ã®å ´åˆ
                                    entry_date = timestamp.date()
                                    entry_datetime = timestamp
                                elif hasattr(timestamp, 'date') and callable(getattr(timestamp, 'date')):
                                    # datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                                    entry_date = timestamp.date()
                                    entry_datetime = timestamp
                                else:
                                    # æ–‡å­—åˆ—ã®å ´åˆ - ã‚ˆã‚Šå®‰å…¨ãªãƒ‘ãƒ¼ã‚¹
                                    try:
                                        if 'T' in str(timestamp):
                                            # ISOå½¢å¼
                                            timestamp_str = str(timestamp).split('.')[0] if '.' in str(timestamp) else str(timestamp)
                                            entry_datetime = datetime.datetime.fromisoformat(timestamp_str)
                                        else:
                                            # é€šå¸¸å½¢å¼
                                            entry_datetime = datetime.datetime.fromisoformat(str(timestamp)[:19])
                                        entry_date = entry_datetime.date()
                                    except Exception as e:
                                        print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (search_page): {e}")
                                        continue
                                
                                # ä»Šæ—¥ã®å­¦ç¿’å•é¡Œã‚’è¨˜éŒ²
                                if entry_date == today:
                                    today_studied_problems.add(q_id)
                                    if not enhanced_data:  # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
                                        today_study_count += 1
                                    if is_hisshu:
                                        today_hisshu_problems.add(q_id)
                                
                                # æ˜¨æ—¥ã®å­¦ç¿’å•é¡Œã‚’è¨˜éŒ²
                                elif entry_date == yesterday:
                                    yesterday_studied_problems.add(q_id)
                                    if not enhanced_data:  # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
                                        yesterday_study_count += 1
                                    if is_hisshu:
                                        yesterday_hisshu_problems.add(q_id)
                                
                                # ç›´è¿‘7æ—¥é–“ã®æ­£è§£ç‡çµ±è¨ˆï¼ˆå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ï¼‰
                                if not enhanced_data and entry_datetime >= seven_days_ago:
                                    recent_7days_stats['total'] += 1
                                    quality = entry.get('quality', 0)
                                    if quality >= 3:
                                        recent_7days_stats['correct'] += 1
                                
                                # å‰ã®7æ—¥é–“ï¼ˆ8-14æ—¥å‰ï¼‰ã®æ­£è§£ç‡çµ±è¨ˆï¼ˆå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ï¼‰
                                elif not enhanced_data and entry_datetime >= fourteen_days_ago:
                                    previous_7days_stats['total'] += 1
                                    quality = entry.get('quality', 0)
                                    if quality >= 3:
                                        previous_7days_stats['correct'] += 1
                            except Exception:
                                # ã™ã¹ã¦ã®ä¾‹å¤–ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                                continue
    
    # ç¾åœ¨ã®ç·å­¦ç¿’æ¸ˆã¿å•é¡Œæ•°ã‚’è¨ˆç®—
    current_studied_count = 0
    current_hisshu_studied_count = 0
    
    # analysis_targetã«åŸºã¥ã„ã¦ç·å•é¡Œæ•°ã‚’æ±ºå®šï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã—ãŸæ­£ç¢ºãªå€¤ï¼‰
    if analysis_target in ["å­¦å£«è©¦é¨“å•é¡Œ", "å­¦å£«è©¦é¨“"]:
        # å­¦å£«è©¦é¨“å•é¡Œã®å ´åˆ: 4,941å•ã€å¿…ä¿®1,100å•
        total_count = 4941
        hisshu_total_count = 1100
    else:
        # å›½è©¦å•é¡Œã®å ´åˆ: 8,576å•ã€å¿…ä¿®1,300å•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        total_count = 8576
        hisshu_total_count = 1300
    
    # UserDataExtractorã‹ã‚‰æ­£ç¢ºãªå­¦ç¿’æ¸ˆã¿æ•°ã‚’å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    if uid and uid != "guest" and UserDataExtractor:
        try:
            extractor = UserDataExtractor()
            comprehensive_stats = extractor.get_user_comprehensive_stats(uid, analysis_target)
            if comprehensive_stats and 'level_distribution' in comprehensive_stats:
                # UserDataExtractorã‹ã‚‰æ­£ç¢ºãªå­¦ç¿’æ¸ˆã¿æ•°ã‚’è¨ˆç®—
                level_dist = comprehensive_stats['level_distribution']
                total_questions = sum(level_dist.values())
                unstudied_count = level_dist.get('æœªå­¦ç¿’', 0)
                current_studied_count = total_questions - unstudied_count
                
                # å¿…ä¿®å•é¡Œã®å­¦ç¿’æ¸ˆã¿æ•°ã‚‚æ­£ç¢ºã«è¨ˆç®—ï¼ˆanalysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸbase_dfã‹ã‚‰ç®—å‡ºï¼‰
                # UserDataExtractorã§ã¯å¿…ä¿®å•é¡Œã®è©³ç´°åˆ¤å®šãŒã§ããªã„ãŸã‚ã€base_dfã‹ã‚‰å†è¨ˆç®—
                for _, row in base_df.iterrows():
                    # analysis_targetã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    row_id = row['id']
                    if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ" and not ("G24" in row_id or "G25" in row_id):
                        continue
                    elif analysis_target == "å›½è©¦å•é¡Œ" and ("G24" in row_id or "G25" in row_id):
                        continue
                    
                    # å¿…ä¿®å•é¡Œã®åˆ¤å®š
                    if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ":
                        # å­¦å£«è©¦é¨“å•é¡Œã®å¿…ä¿®åˆ¤å®š
                        is_hisshu = row_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                    else:
                        # å›½è©¦å•é¡Œã®å¿…ä¿®åˆ¤å®š
                        is_hisshu = row_id in HISSHU_Q_NUMBERS_SET
                    
                    if is_hisshu:
                        card = row['card_data']
                        level = calculate_card_level(card)
                        if level != "æœªå­¦ç¿’":
                            current_hisshu_studied_count += 1
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸbase_dfã‹ã‚‰è¨ˆç®—
                for _, row in base_df.iterrows():
                    # analysis_targetã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    row_id = row['id']
                    if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ" and not ("G24" in row_id or "G25" in row_id):
                        continue
                    elif analysis_target == "å›½è©¦å•é¡Œ" and ("G24" in row_id or "G25" in row_id):
                        continue
                    
                    # å¿…ä¿®å•é¡Œã®åˆ¤å®š
                    if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ":
                        # å­¦å£«è©¦é¨“å•é¡Œã®å¿…ä¿®åˆ¤å®š
                        is_hisshu = row_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                    else:
                        # å›½è©¦å•é¡Œã®å¿…ä¿®åˆ¤å®š
                        is_hisshu = row_id in HISSHU_Q_NUMBERS_SET
                    
                    card = row['card_data']
                    level = calculate_card_level(card)
                    if level != "æœªå­¦ç¿’":
                        current_studied_count += 1
                        if is_hisshu:
                            current_hisshu_studied_count += 1
        except Exception as e:
            print(f"[WARNING] UserDataExtractorå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸbase_dfã‹ã‚‰è¨ˆç®—
            for _, row in base_df.iterrows():
                # analysis_targetã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                row_id = row['id']
                if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ" and not ("G24" in row_id or "G25" in row_id):
                    continue
                elif analysis_target == "å›½è©¦å•é¡Œ" and ("G24" in row_id or "G25" in row_id):
                    continue
                
                # å¿…ä¿®å•é¡Œã®åˆ¤å®š
                if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ":
                    # å­¦å£«è©¦é¨“å•é¡Œã®å¿…ä¿®åˆ¤å®š
                    is_hisshu = row_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                else:
                    # å›½è©¦å•é¡Œã®å¿…ä¿®åˆ¤å®š
                    is_hisshu = row_id in HISSHU_Q_NUMBERS_SET
                
                card = row['card_data']
                level = calculate_card_level(card)
                if level != "æœªå­¦ç¿’":
                    current_studied_count += 1
                    if is_hisshu:
                        current_hisshu_studied_count += 1
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸbase_dfã‹ã‚‰è¨ˆç®—
        for _, row in base_df.iterrows():
            # analysis_targetã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            row_id = row['id']
            if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ" and not ("G24" in row_id or "G25" in row_id):
                continue
            elif analysis_target == "å›½è©¦å•é¡Œ" and ("G24" in row_id or "G25" in row_id):
                continue
            
            # å¿…ä¿®å•é¡Œã®åˆ¤å®š
            if analysis_target == "å­¦å£«è©¦é¨“å•é¡Œ":
                # å­¦å£«è©¦é¨“å•é¡Œã®å¿…ä¿®åˆ¤å®š
                is_hisshu = row_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
            else:
                # å›½è©¦å•é¡Œã®å¿…ä¿®åˆ¤å®š
                is_hisshu = row_id in HISSHU_Q_NUMBERS_SET
            
            card = row['card_data']
            level = calculate_card_level(card)
            if level != "æœªå­¦ç¿’":
                current_studied_count += 1
                if is_hisshu:
                    current_hisshu_studied_count += 1
    
    # æ˜¨æ—¥æ™‚ç‚¹ã§ã®å­¦ç¿’æ¸ˆã¿å•é¡Œæ•°ã‚’æ¨å®šï¼ˆä»Šæ—¥æ–°è¦å­¦ç¿’ã—ãŸå•é¡Œã‚’é™¤ãï¼‰
    yesterday_studied_count = current_studied_count - len(today_studied_problems)
    yesterday_hisshu_studied_count = current_hisshu_studied_count - len(today_hisshu_problems)
    
    # æ­£è§£ç‡è¨ˆç®—ï¼ˆå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆä½¿ç”¨ã€analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    if enhanced_data:
        recent_accuracy = enhanced_data['recent_accuracy']
        previous_accuracy = enhanced_data['previous_accuracy']
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¿…ä¿®ã¨å…¨ä½“ã®æ­£è§£ç‡ã‚‚å–å¾—
        recent_hisshu_stats = enhanced_data.get('recent_hisshu_stats', {'correct': 0, 'total': 0})
        previous_hisshu_stats = enhanced_data.get('previous_hisshu_stats', {'correct': 0, 'total': 0})
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šbase_dfã‹ã‚‰è¨ˆç®—ã™ã‚‹ãŒã€analysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        recent_7days_stats_filtered = {'correct': 0, 'total': 0}
        previous_7days_stats_filtered = {'correct': 0, 'total': 0}
        recent_hisshu_stats = {'correct': 0, 'total': 0}
        previous_hisshu_stats = {'correct': 0, 'total': 0}
        
        for _, row in base_df.iterrows():
            q_id = row['id']
            is_hisshu = row['is_hisshu']
            card = row['card_data']
            history = card.get('history', [])
            
            if isinstance(history, list):
                for entry in history:
                    if isinstance(entry, dict):
                        timestamp = entry.get('timestamp')
                        if timestamp:
                            try:
                                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹
                                if hasattr(timestamp, 'timestamp') and callable(getattr(timestamp, 'timestamp')):
                                    entry_datetime = timestamp
                                elif hasattr(timestamp, 'date') and callable(getattr(timestamp, 'date')):
                                    entry_datetime = timestamp
                                else:
                                    try:
                                        if 'T' in str(timestamp):
                                            timestamp_str = str(timestamp).split('.')[0] if '.' in str(timestamp) else str(timestamp)
                                            entry_datetime = datetime.datetime.fromisoformat(timestamp_str)
                                        else:
                                            entry_datetime = datetime.datetime.fromisoformat(str(timestamp)[:19])
                                    except Exception:
                                        continue
                                
                                quality = entry.get('quality', 0)
                                
                                # ç›´è¿‘7æ—¥é–“ã®æ­£è§£ç‡çµ±è¨ˆï¼ˆanalysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
                                if entry_datetime >= seven_days_ago:
                                    recent_7days_stats_filtered['total'] += 1
                                    if quality >= 3:
                                        recent_7days_stats_filtered['correct'] += 1
                                    
                                    # å¿…ä¿®å•é¡Œã®å ´åˆ
                                    if is_hisshu:
                                        recent_hisshu_stats['total'] += 1
                                        if quality >= 3:
                                            recent_hisshu_stats['correct'] += 1
                                
                                # å‰ã®7æ—¥é–“ï¼ˆ8-14æ—¥å‰ï¼‰ã®æ­£è§£ç‡çµ±è¨ˆï¼ˆanalysis_targetã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
                                elif entry_datetime >= fourteen_days_ago:
                                    previous_7days_stats_filtered['total'] += 1
                                    if quality >= 3:
                                        previous_7days_stats_filtered['correct'] += 1
                                    
                                    # å¿…ä¿®å•é¡Œã®å ´åˆ
                                    if is_hisshu:
                                        previous_hisshu_stats['total'] += 1
                                        if quality >= 3:
                                            previous_hisshu_stats['correct'] += 1
                            except Exception:
                                continue
        
        recent_accuracy = (recent_7days_stats_filtered['correct'] / recent_7days_stats_filtered['total'] * 100) if recent_7days_stats_filtered['total'] > 0 else 0
        previous_accuracy = (previous_7days_stats_filtered['correct'] / previous_7days_stats_filtered['total'] * 100) if previous_7days_stats_filtered['total'] > 0 else 0
    
    # å¿…ä¿®å•é¡Œã®æ­£è§£ç‡ã‚’è¨ˆç®—
    recent_hisshu_accuracy = (recent_hisshu_stats['correct'] / recent_hisshu_stats['total'] * 100) if recent_hisshu_stats['total'] > 0 else 0
    previous_hisshu_accuracy = (previous_hisshu_stats['correct'] / previous_hisshu_stats['total'] * 100) if previous_hisshu_stats['total'] > 0 else 0
    
    # çµæœã‚’æº–å‚™
    result = {
        'current_studied_count': current_studied_count,
        'total_count': total_count,
        'yesterday_studied_count': yesterday_studied_count,
        'progress_delta': current_studied_count - yesterday_studied_count,
        'current_hisshu_studied_count': current_hisshu_studied_count,
        'hisshu_total_count': hisshu_total_count,
        'yesterday_hisshu_studied_count': yesterday_hisshu_studied_count,
        'hisshu_delta': current_hisshu_studied_count - yesterday_hisshu_studied_count,
        'today_study_count': today_study_count,
        'yesterday_study_count': yesterday_study_count,
        'recent_accuracy': recent_accuracy,
        'previous_accuracy': previous_accuracy,
        'accuracy_delta': recent_accuracy - previous_accuracy,
        'recent_hisshu_accuracy': recent_hisshu_accuracy,
        'previous_hisshu_accuracy': previous_hisshu_accuracy,
        'hisshu_accuracy_delta': recent_hisshu_accuracy - previous_hisshu_accuracy,
        'recent_hisshu_stats': recent_hisshu_stats,
        'previous_hisshu_stats': previous_hisshu_stats
    }
    
    # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    cache.set_cached_data(cache_key, result)
    
    return result

def render_search_page():
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜ã«åŸºã¥ãå®Œç’§ãªæ¤œç´¢ãƒ»é€²æ—ãƒšãƒ¼ã‚¸å®Ÿè£…
    
    AI Copilotå‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¦ä»¶ã‚’100%æº€ãŸã™çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½
    """
    
    # â—† ã‚µã‚¤ãƒ‰ãƒãƒ¼é€£æºï¼šanalysis_target (å›½è©¦/å­¦å£«è©¦é¨“) ã®å–å¾—
    analysis_target = st.session_state.get("analysis_target", "å›½è©¦")
    level_filter = st.session_state.get("level_filter", ["æœªå­¦ç¿’", "ãƒ¬ãƒ™ãƒ«0", "ãƒ¬ãƒ™ãƒ«1", "ãƒ¬ãƒ™ãƒ«2", "ãƒ¬ãƒ™ãƒ«3", "ãƒ¬ãƒ™ãƒ«4", "ç¿’å¾—æ¸ˆã¿"])
    subject_filter = st.session_state.get("subject_filter", [])
    
    # 1. æ¦‚è¦ã¨ç›®çš„ - ãƒšãƒ¼ã‚¸ãƒ˜ãƒƒãƒ€ãƒ¼
    st.subheader(f"ğŸ“ˆ å­¦ç¿’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ({analysis_target})")
    
    # 2. åˆæœŸãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ï¼‰
    uid = st.session_state.get("uid", "guest")
    cards = st.session_state.get("cards", {})
    
    # ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ³ã‚’ç¢ºèª
    print(f"[DEBUG] UID: {uid}")
    print(f"[DEBUG] Cardsæ•°: {len(cards) if cards else 0}")
    print(f"[DEBUG] Analysis target: {analysis_target}")
    
    # å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    if uid == "guest" and not cards:
        print(f"[DEBUG] ã‚²ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŸã‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™")
        st.info("ğŸ“Š ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã—ã¾ã™ï¼ˆãƒ­ã‚°ã‚¤ãƒ³å¾Œã¯å®Ÿéš›ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«è‡ªå‹•æ›´æ–°ã•ã‚Œã¾ã™ï¼‰")
        test_cards = generate_test_cards_data(200)  # 200ä»¶ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        cards.update(test_cards)
        st.session_state["cards"] = cards
        print(f"[DEBUG] ãƒ†ã‚¹ãƒˆã‚«ãƒ¼ãƒ‰æ•°: {len(test_cards)}")
    elif cards:
        print(f"[DEBUG] æ—¢å­˜ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(cards)}ä»¶")
        
        # ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’question_idã§ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
        question_id_to_card = {}
        for card_key, card_data in cards.items():
            if isinstance(card_data, dict):
                question_id = card_data.get('question_id')
                if question_id and question_id != card_key:
                    question_id_to_card[question_id] = card_data
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’cardsã«è¿½åŠ 
        cards.update(question_id_to_card)
        st.session_state["cards"] = cards
        print(f"[DEBUG] question_idã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ å¾Œã®ã‚«ãƒ¼ãƒ‰æ•°: {len(cards)}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ
    cache = SearchPageCache()
    cache_key = f"user_cards_{uid}"
    cached_cards = cache.get_cached_data(cache_key)
    
    # uidãŒå­˜åœ¨ã—ã€cardsãŒç©ºã®å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¾ãŸã¯Firestoreã‹ã‚‰èª­ã¿è¾¼ã¿
    if uid != "guest" and not cards:
        if cached_cards is not None:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            cards.update(cached_cards)
            st.session_state["cards"] = cards
        else:
            # Firestoreã‹ã‚‰æ–°è¦å–å¾—
            try:
                db = get_firestore_manager()
                user_cards = db.get_user_cards(uid)
                if user_cards:
                    cards.update(user_cards)
                    st.session_state["cards"] = cards
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    cache.set_cached_data(cache_key, user_cards)
                
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å–å¾—ã—ã¦æ¼”ç¿’ãƒ­ã‚°ã‚’ç¢ºèª
                    try:
                        user_ref = db.db.collection("users").document(uid)
                        user_doc = user_ref.get()
                        
                        if user_doc.exists:
                            user_data = user_doc.to_dict()
                            result_log = user_data.get('result_log', {})
                            
                            if result_log:
                                # result_logã‚’historyã«å¤‰æ›
                                for q_id, log_entry in result_log.items():
                                    if q_id in cards:
                                        if 'history' not in cards[q_id]:
                                            cards[q_id]['history'] = []
                                        
                                        # ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’historyå½¢å¼ã«å¤‰æ›
                                        history_entry = {
                                            'timestamp': log_entry.get('timestamp'),
                                            'quality': log_entry.get('quality', 0),
                                            'is_correct': log_entry.get('quality', 0) >= 3,
                                            'user_answer': log_entry.get('user_answer'),
                                            'time_spent': log_entry.get('time_spent')
                                        }
                                        cards[q_id]['history'].append(history_entry)
                            
                    except Exception as e:
                        print(f"[WARNING] result_logå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        
            except Exception as e:
                st.error(f"[ERROR] Firestoreå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"[WARNING] Firestoreå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®result_logã‚‚ç¢ºèª
    result_log = st.session_state.get("result_log", {})
    if result_log:
        # result_logã‹ã‚‰historyã‚’ä½œæˆ
        for q_id, log_entry in result_log.items():
            if q_id in cards:
                if 'history' not in cards[q_id]:
                    cards[q_id]['history'] = []
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®result_logã‹ã‚‰historyå½¢å¼ã«å¤‰æ›
                history_entry = {
                    'timestamp': log_entry.get('timestamp'),
                    'quality': log_entry.get('quality', 0),
                    'is_correct': log_entry.get('quality', 0) >= 3,
                    'user_answer': log_entry.get('user_answer'),
                    'time_spent': log_entry.get('time_spent')
                }
                cards[q_id]['history'].append(history_entry)
    
    # 3. æ¨©é™ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã®å–å¾—
    has_gakushi_permission = check_gakushi_permission(uid)
    analysis_target = st.session_state.get("analysis_target", "å›½è©¦")
    level_filter = st.session_state.get("level_filter", LEVEL_ORDER)
    subject_filter = st.session_state.get("subject_filter", [])
    
    # 4. 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã«åŸºã¥ãä¿®æ­£ï¼šä¸»è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€åº¦ã ã‘ä½œæˆ
    all_data = []
    
    # UserDataExtractorã‹ã‚‰ç›´æ¥å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    user_data_extractor = None
    actual_cards_data = {}
    
    try:
        from my_llm_app.user_data_extractor import UserDataExtractor
        user_data_extractor = UserDataExtractor()
        if uid != "guest":
            # UserDataExtractorã‹ã‚‰å®Ÿéš›ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            user_stats = user_data_extractor.get_comprehensive_statistics(uid, force_refresh=True)
            if user_stats and 'card_levels' in user_stats:
                actual_cards_data = user_stats['card_levels']
                print(f"[DEBUG] UserDataExtractorã‹ã‚‰å–å¾—ã—ãŸã‚«ãƒ¼ãƒ‰æ•°: {len(actual_cards_data)}")
            else:
                print(f"[DEBUG] UserDataExtractorã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
    except Exception as e:
        print(f"[DEBUG] UserDataExtractorå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã¨å•é¡Œãƒ‡ãƒ¼ã‚¿ã®ç´ä»˜ã‘ã®ãŸã‚ã®æº–å‚™
    question_id_to_card_mapping = {}
    
    # 1. UserDataExtractorã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆä½¿ç”¨
    for card_id, card_data in actual_cards_data.items():
        if isinstance(card_data, dict):
            question_id = card_data.get('question_id', card_id)
            question_id_to_card_mapping[question_id] = card_data
    
    # 2. ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã¨ã—ã¦ä½¿ç”¨
    for card_id, card_data in cards.items():
        if isinstance(card_data, dict):
            question_id = card_data.get('question_id', card_id)
            if question_id not in question_id_to_card_mapping:
                question_id_to_card_mapping[question_id] = card_data
    
    print(f"[DEBUG] æœ€çµ‚å•é¡ŒID->ã‚«ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°æ•°: {len(question_id_to_card_mapping)}")
    print(f"[DEBUG] å®Ÿéš›ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿æ•°: {len(actual_cards_data)}")
    print(f"[DEBUG] ãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹: {list(question_id_to_card_mapping.keys())[:5]}")
    
    for question in ALL_QUESTIONS:
        q_number = question.get('number', '')
        
        # analysis_targetã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ã«åŸºã¥ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if analysis_target in ["å›½è©¦", "å›½è©¦å•é¡Œ"] and q_number.startswith('G'):
            continue
        if analysis_target in ["å­¦å£«è©¦é¨“", "å­¦å£«è©¦é¨“å•é¡Œ"]:
            if not q_number.startswith('G') or not has_gakushi_permission:
                continue
        
        # å„å•é¡Œã«å¯¾å¿œã™ã‚‹cardsãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
        card = question_id_to_card_mapping.get(q_number, {})
        level = calculate_card_level(card)
        
        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®æ•°ä»¶ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›
        if len(all_data) < 3:
            print(f"[DEBUG] card[{len(all_data)}] q_number: {q_number}")
            print(f"[DEBUG] card[{len(all_data)}] card_found: {bool(card)}")
            if card:
                print(f"[DEBUG] card[{len(all_data)}] raw_level: {card.get('level')}")
                print(f"[DEBUG] card[{len(all_data)}] mastery_status: {card.get('mastery_status')}")
            print(f"[DEBUG] card[{len(all_data)}] calculated_level: {level}")
        
        # is_hisshuãƒ•ãƒ©ã‚°ã‚’analysis_targetã«å¿œã˜ã¦åˆ¤å®š
        if analysis_target in ["å­¦å£«è©¦é¨“", "å­¦å£«è©¦é¨“å•é¡Œ"]:
            is_hisshu = q_number in GAKUSHI_HISSHU_Q_NUMBERS_SET
        else:
            is_hisshu = q_number in HISSHU_Q_NUMBERS_SET
        
        all_data.append({
            'id': q_number,
            'subject': question.get('subject', ''),
            'year': question.get('year', 0),
            'question_text': question.get('question_text', ''),
            'choices': question.get('choices', []),
            'answer': question.get('answer', ''),
            'level': level,
            'is_hisshu': is_hisshu,
            'card_data': card,
            'history': card.get('history', [])
        })
    
    # åŸºæœ¬DataFrameã‚’ä½œæˆï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ã®å…¨å¯¾è±¡å•é¡Œï¼‰
    base_df = pd.DataFrame(all_data)
    
    # 5. ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’åŸºæœ¬DataFrameã«é©ç”¨
    filtered_df = base_df.copy()
    
    if not filtered_df.empty:
        # ãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        if level_filter and set(level_filter) != set(LEVEL_ORDER):
            filtered_df = filtered_df[filtered_df['level'].isin(level_filter)]
        
        # ç§‘ç›®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        if subject_filter:
            filtered_df = filtered_df[filtered_df['subject'].isin(subject_filter)]
        
        # å¿…ä¿®å•é¡Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        show_hisshu_only = st.session_state.get('show_hisshu_only', False)
        if show_hisshu_only:
            filtered_df = filtered_df[filtered_df['is_hisshu'] == True]
    
    # 6. ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—ã¨è¡¨ç¤ºï¼ˆUserDataExtractorå¼·åŒ–ç‰ˆï¼‰
    if not filtered_df.empty:
        # æ–°ã—ã„actionableãªæŒ‡æ¨™ã®è¨ˆç®—ï¼ˆå‰æ—¥æ¯”ãƒ»å‰é€±æ¯”ã‚’å«ã‚€ï¼‰
        metrics = calculate_progress_metrics(cards, base_df, uid, analysis_target)
        
        # UserDataExtractorã‹ã‚‰ã®è¿½åŠ æƒ…å ±ã‚’å–å¾—
        extractor_insights = {}
        if uid != "guest" and UserDataExtractor:
            try:
                extractor = UserDataExtractor()
                comprehensive_stats = extractor.get_user_comprehensive_stats(uid, analysis_target)
                if comprehensive_stats:
                    extractor_insights = {
                        'weak_categories': comprehensive_stats.get('weak_categories', []),
                        'learning_efficiency': comprehensive_stats.get('learning_efficiency', 0),
                        'total_studied_cards': comprehensive_stats.get('total_studied_cards', 0)
                    }
            except Exception as e:
                print(f"[WARNING] UserDataExtractor insightså–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        # st.columns(4)ã‚’ä½¿ç”¨ã—ã¦4ã¤ã®æ–°ã—ã„æŒ‡æ¨™ã‚’st.metricã§è¡¨ç¤º
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            # å­¦ç¿’é€²æ—ç‡ï¼ˆå‰æ—¥æ¯”ä»˜ãï¼‰
            progress_delta_text = f"+{metrics['progress_delta']} å•ï¼ˆå‰æ—¥æ¯”ï¼‰" if metrics['progress_delta'] > 0 else f"{metrics['progress_delta']} å•ï¼ˆå‰æ—¥æ¯”ï¼‰" if metrics['progress_delta'] < 0 else "å¤‰åŒ–ãªã—ï¼ˆå‰æ—¥æ¯”ï¼‰"
            st.metric(
                "å­¦ç¿’é€²æ—ç‡",
                f"{metrics['current_studied_count']} / {metrics['total_count']} å•",
                delta=progress_delta_text
            )
            
            # å¼±ç‚¹åˆ†é‡ã®ãƒ’ãƒ³ãƒˆè¡¨ç¤º
            if extractor_insights.get('weak_categories'):
                weak_hint = ", ".join(extractor_insights['weak_categories'][:2])
                st.caption(f"ğŸ’¡ è¦å¾©ç¿’: {weak_hint}")
        
        with col2:
            # å¿…ä¿®å•é¡Œã®é€²æ—ï¼ˆå‰æ—¥æ¯”ä»˜ãï¼‰
            hisshu_delta_text = f"+{metrics['hisshu_delta']} å•ï¼ˆå‰æ—¥æ¯”ï¼‰" if metrics['hisshu_delta'] > 0 else f"{metrics['hisshu_delta']} å•ï¼ˆå‰æ—¥æ¯”ï¼‰" if metrics['hisshu_delta'] < 0 else "å¤‰åŒ–ãªã—ï¼ˆå‰æ—¥æ¯”ï¼‰"
            st.metric(
                "å¿…ä¿®å•é¡Œã®é€²æ—",
                f"{metrics['current_hisshu_studied_count']} / {metrics['hisshu_total_count']} å•",
                delta=hisshu_delta_text
            )
        
        with col3:
            # ä»Šæ—¥ã®å­¦ç¿’ï¼ˆæ˜¨æ—¥ã®å®Ÿç¸¾æ¯”è¼ƒä»˜ãï¼‰
            today_delta_text = f"æ˜¨æ—¥: {metrics['yesterday_study_count']} å•"
            st.metric(
                "ä»Šæ—¥ã®å­¦ç¿’",
                f"{metrics['today_study_count']} å•",
                delta=today_delta_text
            )
            
            # å­¦ç¿’åŠ¹ç‡ã‚¹ã‚³ã‚¢è¡¨ç¤º
            if extractor_insights.get('learning_efficiency', 0) > 0:
                efficiency = extractor_insights['learning_efficiency']
                st.caption(f"ğŸ“ˆ å­¦ç¿’åŠ¹ç‡: {efficiency:.1%}")
        
        with col4:
            # ç›´è¿‘7æ—¥é–“ã®æ­£è§£ç‡ï¼ˆå‰é€±æ¯”ä»˜ãï¼‰
            accuracy_delta_text = f"{metrics['accuracy_delta']:+.1f}%ï¼ˆå‰é€±æ¯”ï¼‰"
            delta_color = "normal" if metrics['accuracy_delta'] >= 0 else "inverse"
            st.metric(
                "ç›´è¿‘7æ—¥é–“ã®æ­£è§£ç‡",
                f"{metrics['recent_accuracy']:.1f} %",
                delta=accuracy_delta_text,
                delta_color=delta_color
            )
    
    # 7. ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒŠ - 4ã¤ã®ã‚¿ãƒ–ï¼ˆè©³ç´°åˆ†æã‚¿ãƒ–ã‚’å‰Šé™¤ã—ã¦å…ƒã®æ§‹æˆã«æˆ»ã™ï¼‰
    tab1, tab2, tab3, tab4 = st.tabs(["æ¦‚è¦", "ã‚°ãƒ©ãƒ•åˆ†æ", "å•é¡Œãƒªã‚¹ãƒˆ", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"])
    
    with tab1:
        render_overview_tab_perfect(filtered_df, ALL_QUESTIONS, analysis_target)
    
    with tab2:
        render_graph_analysis_tab_perfect(filtered_df)
    
    with tab3:
        render_question_list_tab_perfect(filtered_df, analysis_target)
    
    with tab4:
        render_keyword_search_tab_perfect(analysis_target)

def render_overview_tab_perfect(filtered_df: pd.DataFrame, ALL_QUESTIONS: list, analysis_target: str):
    """
    æ¦‚è¦ã‚¿ãƒ– - UserDataExtractorå¼·åŒ–ç‰ˆ
    st.columns(2)ã§2åˆ†å‰²ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€ç¿’ç†Ÿåº¦åˆ†å¸ƒã¨æ­£è§£ç‡è¡¨ç¤º
    """
    if filtered_df.empty:
        st.info("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # UserDataExtractorã‹ã‚‰ã®è¿½åŠ æ´å¯Ÿã‚’å–å¾—
    uid = st.session_state.get("uid", "guest")
    insights_text = ""
    if uid != "guest" and UserDataExtractor:
        try:
            extractor = UserDataExtractor()
            comprehensive_stats = extractor.get_user_comprehensive_stats(uid)
            if comprehensive_stats:
                weak_areas = comprehensive_stats.get('weak_categories', [])
                efficiency = comprehensive_stats.get('learning_efficiency', 0)
                if weak_areas:
                    insights_text = f"ğŸ’¡ æ¨å¥¨å¾©ç¿’åˆ†é‡: {', '.join(weak_areas[:3])}"
                if efficiency > 0.7:
                    insights_text += " | ğŸš€ å­¦ç¿’åŠ¹ç‡ãŒè‰¯å¥½ã§ã™"
        except Exception as e:
            print(f"[WARNING] æ¦‚è¦ã‚¿ãƒ–æ´å¯Ÿå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # st.columns(2)ã‚’ä½¿ç”¨ã—ã¦2åˆ†å‰²ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### ã‚«ãƒ¼ãƒ‰ç¿’ç†Ÿåº¦åˆ†å¸ƒ")
        
        # UserDataExtractorã‹ã‚‰ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
        level_distribution_source = "å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯"
        level_counts = None
        
        if uid != "guest" and UserDataExtractor:
            try:
                extractor = UserDataExtractor()
                enhanced_stats = extractor.get_user_comprehensive_stats(uid, analysis_target)
                if enhanced_stats and 'level_distribution' in enhanced_stats:
                    level_dist = enhanced_stats['level_distribution']
                    
                    # UserDataExtractorã®çµæœã‚’ä½¿ç”¨
                    level_counts = pd.Series(level_dist)
                    level_counts = level_counts.reindex(LEVEL_ORDER, fill_value=0)
                    level_distribution_source = "UserDataExtractor"
            except Exception as e:
                print(f"[WARNING] UserDataExtractor ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯
        if level_counts is None:
            updated_levels = []
            for _, row in filtered_df.iterrows():
                card_data = row['card_data']
                updated_level = calculate_card_level(card_data)
                updated_levels.append(updated_level)
            
            level_counts = pd.Series(updated_levels).value_counts()
            level_counts = level_counts.reindex(LEVEL_ORDER, fill_value=0)
        
        # è¡¨å½¢å¼è¡¨ç¤º
        level_df = pd.DataFrame({
            'ãƒ¬ãƒ™ãƒ«': level_counts.index,
            'å•é¡Œæ•°': level_counts.values
        })
        
        st.dataframe(
            level_df,
            use_container_width=True,
            hide_index=True
        )
        
        # AIæ´å¯Ÿã‚’è¡¨ç¤º
        if insights_text:
            st.info(insights_text)
    
    with col2:
        st.markdown("##### æ­£è§£ç‡ (True Retention)")
        
        # UserDataExtractorã‹ã‚‰ã‚ˆã‚Šæ­£ç¢ºãªæ­£è§£ç‡ã‚’å–å¾—
        uid = st.session_state.get("uid", "guest")
        enhanced_accuracy = None
        if uid != "guest" and UserDataExtractor:
            try:
                extractor = UserDataExtractor()
                evaluation_logs = extractor.extract_self_evaluation_logs(uid)
                if evaluation_logs:
                    # analysis_targetã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filtered_logs = []
                    for log in evaluation_logs:
                        q_id = log.get('question_id', '')
                        # å­¦å£«è©¦é¨“å•é¡Œã‹ã©ã†ã‹ã®åˆ¤å®šã‚’çµ±ä¸€
                        if analysis_target in ["å­¦å£«è©¦é¨“å•é¡Œ", "å­¦å£«è©¦é¨“"]:
                            # å­¦å£«è©¦é¨“å•é¡Œã®ã¿ï¼ˆGã§å§‹ã¾ã‚‹ï¼‰
                            if q_id.startswith('G'):
                                filtered_logs.append(log)
                        else:
                            # å›½è©¦å•é¡Œã®ã¿ï¼ˆGã§å§‹ã¾ã‚‰ãªã„ï¼‰
                            if not q_id.startswith('G'):
                                filtered_logs.append(log)
                    
                    print(f"[INFO] {analysis_target}ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(filtered_logs)}ä»¶ (å…ƒ: ç·{len(evaluation_logs)}ä»¶)")
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ­ã‚°ã§æ­£è§£ç‡è¨ˆç®—
                    if filtered_logs:
                        # å…¨ä½“æ­£è§£ç‡
                        total_correct = sum(1 for log in filtered_logs if log.get('quality', 0) >= 3)
                        total_attempts = len(filtered_logs)
                        overall_rate = (total_correct / total_attempts * 100) if total_attempts > 0 else 0
                        
                        # å¿…ä¿®å•é¡Œæ­£è§£ç‡
                        hisshu_correct = 0
                        hisshu_attempts = 0
                        
                        # analysis_targetã«å¿œã˜ã¦é©åˆ‡ãªå¿…ä¿®å•é¡Œã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
                        for log in filtered_logs:
                            q_id = log.get('question_id', '')
                            is_hisshu = False
                            
                            if analysis_target in ["å­¦å£«è©¦é¨“å•é¡Œ", "å­¦å£«è©¦é¨“"]:
                                # å­¦å£«è©¦é¨“å•é¡Œ: GAKUSHI_HISSHU_Q_NUMBERS_SETã‚’ä½¿ç”¨
                                is_hisshu = q_id in GAKUSHI_HISSHU_Q_NUMBERS_SET
                            else:
                                # å›½è©¦å•é¡Œ: HISSHU_Q_NUMBERS_SETã‚’ä½¿ç”¨
                                is_hisshu = q_id in HISSHU_Q_NUMBERS_SET
                            
                            if is_hisshu:
                                hisshu_attempts += 1
                                if log.get('quality', 0) >= 3:
                                    hisshu_correct += 1
                        
                        hisshu_rate = (hisshu_correct / hisshu_attempts * 100) if hisshu_attempts > 0 else 0
                        
                        enhanced_accuracy = {
                            'overall_rate': overall_rate,
                            'overall_attempts': total_attempts,
                            'overall_correct': total_correct,
                            'hisshu_rate': hisshu_rate,
                            'hisshu_attempts': hisshu_attempts,
                            'hisshu_correct': hisshu_correct
                        }
                        print(f"[INFO] æ¦‚è¦ã‚¿ãƒ–å¼·åŒ–: å…¨ä½“æ­£è§£ç‡{overall_rate:.1f}%, å¿…ä¿®æ­£è§£ç‡{hisshu_rate:.1f}%")
            except Exception as e:
                print(f"[WARNING] æ¦‚è¦ã‚¿ãƒ–æ­£è§£ç‡å¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if enhanced_accuracy:
            # UserDataExtractorã‹ã‚‰ã®é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            st.metric(
                label="é¸æŠç¯„å›²ã®æ­£è§£ç‡",
                value=f"{enhanced_accuracy['overall_rate']:.1f}%",
                delta=f"{enhanced_accuracy['overall_correct']} / {enhanced_accuracy['overall_attempts']} å›"
            )
            st.metric(
                label="ã€å¿…ä¿®å•é¡Œã€‘ã®æ­£è§£ç‡",
                value=f"{enhanced_accuracy['hisshu_rate']:.1f}%",
                delta=f"{enhanced_accuracy['hisshu_correct']} / {enhanced_accuracy['hisshu_attempts']} å›"
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯
            total_correct = 0
            total_attempts = 0
            hisshu_correct = 0
            hisshu_attempts = 0
            
            for _, row in filtered_df.iterrows():
                history = row.get('history', [])
                is_hisshu = row.get('is_hisshu', False)
                
                if isinstance(history, list):
                    for entry in history:
                        if isinstance(entry, dict):
                            # qualityå€¤ã«ã‚ˆã‚‹æ­£è§£åˆ¤å®šï¼ˆquality >= 3ã§æ­£è§£ï¼‰
                            quality = entry.get('quality', 0)
                            is_correct = quality >= 3
                            
                            total_attempts += 1
                            if is_correct:
                                total_correct += 1
                            
                            if is_hisshu:
                                hisshu_attempts += 1
                                if is_correct:
                                    hisshu_correct += 1
            
            # æ­£è§£ç‡è¨ˆç®—
            overall_rate = (total_correct / total_attempts * 100) if total_attempts > 0 else 0
            hisshu_rate = (hisshu_correct / hisshu_attempts * 100) if hisshu_attempts > 0 else 0
            
            # st.metricã‚’2ã¤ä½¿ç”¨ï¼ˆdeltaå¼•æ•°ã§å†…è¨³ã‚’è¡¨ç¤ºï¼‰
            st.metric(
                label="é¸æŠç¯„å›²ã®æ­£è§£ç‡",
                value=f"{overall_rate:.1f}%",
                delta=f"{total_correct} / {total_attempts} å›"
            )
            st.metric(
                label="ã€å¿…ä¿®å•é¡Œã€‘ã®æ­£è§£ç‡",
                value=f"{hisshu_rate:.1f}%",
                delta=f"{hisshu_correct} / {hisshu_attempts} å›"
            )

def render_graph_analysis_tab_perfect(filtered_df: pd.DataFrame):
    """
    ã‚°ãƒ©ãƒ•åˆ†æã‚¿ãƒ– - UserDataExtractorå¼·åŒ–ç‰ˆ
    ç§‘ç›®åˆ¥é€²æ—ã€å­¦ç¿’è¨˜éŒ²ã€ãƒ¬ãƒ™ãƒ«åˆ¥åˆ†å¸ƒã‚’Plotlyã§è¡¨ç¤º
    """
    if filtered_df.empty:
        st.info("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # UserDataExtractorã‹ã‚‰ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    uid = st.session_state.get("uid", "guest")
    enhanced_analytics = {}
    if uid != "guest" and UserDataExtractor:
        try:
            extractor = UserDataExtractor()
            evaluation_logs = extractor.extract_self_evaluation_logs(uid)
            if evaluation_logs:
                enhanced_analytics['evaluation_logs'] = evaluation_logs
                print(f"[INFO] ã‚°ãƒ©ãƒ•åˆ†æå¼·åŒ–: {len(evaluation_logs)}ä»¶ã®è©•ä¾¡ãƒ­ã‚°ã‚’å–å¾—")
        except Exception as e:
            print(f"[WARNING] ã‚°ãƒ©ãƒ•åˆ†æå¼·åŒ–ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ç§‘ç›®åˆ¥é€²æ—
    st.markdown("##### ç§‘ç›®åˆ¥é€²æ—çŠ¶æ³")
    
    # ã‚°ãƒ©ãƒ•è¡¨ç¤ºå‰ã®èª¬æ˜ã‚’è¿½åŠ 
    st.info("ğŸ“Š å„ç§‘ç›®ã®å­¦ç¿’é€²æ—ã‚’æ¨ªæ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚°ãƒ¬ãƒ¼: æœªå­¦ç¿’ã€é’: å­¦ç¿’ä¸­ã€ç·‘: ç¿’å¾—æ¸ˆã¿")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
    print(f"[DEBUG] filtered_df shape: {filtered_df.shape}")
    print(f"[DEBUG] filtered_df columns: {filtered_df.columns.tolist()}")
    if not filtered_df.empty:
        print(f"[DEBUG] unique subjects: {filtered_df['subject'].unique()}")
        print(f"[DEBUG] unique levels: {filtered_df['level'].unique()}")
    else:
        print(f"[DEBUG] filtered_df is empty!")
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«Streamlitã§ã‚‚æƒ…å ±ã‚’è¡¨ç¤º
    if st.checkbox("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º", key="debug_graph"):
        st.write(f"ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(filtered_df)}")
        if not filtered_df.empty:
            st.write(f"ç§‘ç›®æ•°: {len(filtered_df['subject'].unique())}")
            st.write(f"ç§‘ç›®ä¸€è¦§: {list(filtered_df['subject'].unique())}")
            st.write(f"ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ:")
            st.write(filtered_df['level'].value_counts())
            
            # å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            st.write("**ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®5è¡Œï¼‰:**")
            st.dataframe(filtered_df[['id', 'subject', 'level']].head())
    
    # ã‚°ãƒ©ãƒ•ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã®ä»£æ›¿è¡¨ç¤º
    if st.checkbox("ç°¡æ˜“ç‰ˆã‚°ãƒ©ãƒ•ã‚’å¼·åˆ¶è¡¨ç¤º", key="force_simple_graph"):
        st.write("**ç°¡æ˜“ç‰ˆã‚°ãƒ©ãƒ•ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰:**")
        if not filtered_df.empty and 'subject' in filtered_df.columns:
            # æ¦‚è¦ã‚¿ãƒ–ã¨åŒã˜æ–¹æ³•ã§ãƒ¬ãƒ™ãƒ«ã‚’å†è¨ˆç®—
            corrected_data = []
            for _, row in filtered_df.iterrows():
                card_data = row['card_data']
                actual_level = calculate_card_level(card_data)
                corrected_data.append({
                    'subject': row['subject'],
                    'level': actual_level
                })
            
            corrected_df = pd.DataFrame(corrected_data)
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªç§‘ç›®åˆ¥ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ
            subject_level_pivot = corrected_df.groupby(['subject', 'level']).size().unstack(fill_value=0)
            if not subject_level_pivot.empty:
                st.bar_chart(subject_level_pivot)
            else:
                st.warning("ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã™")
        else:
            st.error("å¿…è¦ãªåˆ—ï¼ˆsubjectï¼‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    try:
        # ç§‘ç›®åˆ¥ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ã«é›†è¨ˆï¼ˆå®Ÿéš›ã®JSONãƒ‡ãƒ¼ã‚¿ã®ç§‘ç›®åã‚’ä½¿ç”¨ï¼‰
        subject_level_data = []
        
        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
        if filtered_df.empty:
            st.warning("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        # ç§‘ç›®ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        available_subjects = filtered_df['subject'].dropna().unique()
        available_subjects = [s for s in available_subjects if s and s.strip()]  # ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
        
        if len(available_subjects) == 0:
            st.warning("âš ï¸ æœ‰åŠ¹ãªç§‘ç›®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            st.info("å¯èƒ½æ€§ã®ã‚ã‚‹åŸå› : å•é¡Œãƒ‡ãƒ¼ã‚¿ã«ç§‘ç›®æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒã®ã¿è¡¨ç¤º
            st.markdown("##### ãƒ¬ãƒ™ãƒ«åˆ¥å­¦ç¿’çŠ¶æ³ï¼ˆå…¨å•é¡Œå¯¾è±¡ï¼‰")
            if not filtered_df.empty:
                # æ¦‚è¦ã‚¿ãƒ–ã¨åŒã˜æ–¹æ³•ã§ãƒ¬ãƒ™ãƒ«ã‚’å†è¨ˆç®—
                actual_levels = []
                for _, row in filtered_df.iterrows():
                    card_data = row['card_data']
                    actual_level = calculate_card_level(card_data)
                    actual_levels.append(actual_level)
                
                level_counts = pd.Series(actual_levels).value_counts()
                if len(level_counts) > 0:
                    # ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                    fig_pie = px.pie(
                        values=level_counts.values,
                        names=level_counts.index,
                        title="å­¦ç¿’ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ",
                        color_discrete_map={
                            'æœªå­¦ç¿’': '#BDBDBD',
                            'ãƒ¬ãƒ™ãƒ«0': '#E3F2FD',
                            'ãƒ¬ãƒ™ãƒ«1': '#BBDEFB',
                            'ãƒ¬ãƒ™ãƒ«2': '#90CAF9',
                            'ãƒ¬ãƒ™ãƒ«3': '#64B5F6',
                            'ãƒ¬ãƒ™ãƒ«4': '#42A5F5',
                            'ãƒ¬ãƒ™ãƒ«5': '#2196F3',
                            'ç¿’å¾—æ¸ˆã¿': '#4CAF50'
                        }
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
                    
                    # ç°¡æ˜“ç‰ˆã®æ£’ã‚°ãƒ©ãƒ•ã‚‚è¡¨ç¤º
                    st.bar_chart(level_counts)
                else:
                    st.info("ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        print(f"[DEBUG] æœ‰åŠ¹ãªç§‘ç›®æ•°: {len(available_subjects)}")
        print(f"[DEBUG] æœ‰åŠ¹ãªç§‘ç›®: {available_subjects}")
        
        for subject in available_subjects:
            # å®Ÿéš›ã®ç§‘ç›®åã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ¨™æº–åŒ–ã¯è¡Œã‚ãªã„ï¼‰
            subject_df = filtered_df[filtered_df['subject'] == subject]
            total_count = len(subject_df)
            
            if total_count == 0:
                print(f"[DEBUG] ç§‘ç›® '{subject}' ã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã€ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # æ¦‚è¦ã‚¿ãƒ–ã¨åŒã˜æ–¹æ³•ã§ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
            actual_levels = []
            for _, row in subject_df.iterrows():
                card_data = row['card_data']
                actual_level = calculate_card_level(card_data)
                actual_levels.append(actual_level)
            
            # å„ãƒ¬ãƒ™ãƒ«ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå®Ÿéš›ã«è¨ˆç®—ã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«ï¼‰
            level_counts = pd.Series(actual_levels).value_counts()
            print(f"[DEBUG] ç§‘ç›® '{subject}' ã®å®Ÿéš›ã®ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {dict(level_counts)}")
            
            # æœªå­¦ç¿’ä»¥å¤–ã‚’ã€Œå­¦ç¿’æ¸ˆã¿ã€ã¨ã—ã¦é›†è¨ˆ
            learned_count = total_count - level_counts.get('æœªå­¦ç¿’', 0)
            mastered_count = level_counts.get('ç¿’å¾—æ¸ˆã¿', 0)
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—
            learned_pct = (learned_count / total_count * 100) if total_count > 0 else 0
            mastered_pct = (mastered_count / total_count * 100) if total_count > 0 else 0
            unlearned_pct = 100 - learned_pct
            
            subject_level_data.append({
                'subject': subject,  # å®Ÿéš›ã®ç§‘ç›®åã‚’ãã®ã¾ã¾ä½¿ç”¨
                'total': total_count,
                'learned': learned_count,
                'mastered': mastered_count,
                'learned_pct': learned_pct,
                'mastered_pct': mastered_pct,
                'unlearned_pct': unlearned_pct
            })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        progress_df = pd.DataFrame(subject_level_data)
        
        print(f"[DEBUG] subject_level_data length: {len(subject_level_data)}")
        print(f"[DEBUG] progress_df shape: {progress_df.shape}")
        
        if len(progress_df) == 0:
            st.warning("âš ï¸ ç§‘ç›®åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œãªã„åŸå› :")
            st.info("â€¢ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã«ã‚ˆã‚Šã€è¡¨ç¤ºå¯¾è±¡ã®å•é¡ŒãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            st.info("â€¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            st.info("â€¢ é¸æŠä¸­ã®è©¦é¨“ç¨®åˆ¥ï¼ˆå›½è©¦/å­¦å£«è©¦é¨“ï¼‰ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return
        
        # å®Ÿéš›ã®ç§‘ç›®åã‚’ä½¿ç”¨ã™ã‚‹ã®ã§é‡è¤‡çµ±åˆã¯ä¸è¦
        # å•é¡Œæ•°ã§é™é †ã‚½ãƒ¼ãƒˆã—ã¦è¦‹ã‚„ã™ãã™ã‚‹
        if len(progress_df) > 0:
            progress_df = progress_df.sort_values('total', ascending=True)  # å•é¡Œæ•°æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
        
        # ç©ã¿ä¸Šã’æ¨ªæ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        print(f"[DEBUG] ã‚°ãƒ©ãƒ•ä½œæˆé–‹å§‹ - ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(progress_df)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if progress_df.empty:
            st.error("ã‚°ãƒ©ãƒ•ç”¨ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return
            
        # å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        required_columns = ['subject', 'unlearned_pct', 'learned_pct', 'mastered_pct']
        missing_columns = [col for col in required_columns if col not in progress_df.columns]
        if missing_columns:
            st.error(f"å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")
            return
        
        fig = go.Figure()
        
        # å¯è¦–æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€æœ€å°è¡¨ç¤ºå¹…ã‚’è¨­å®š
        min_visible_width = 2.0  # æœ€ä½2%ã¯è¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
        
        print(f"[DEBUG] progress_dfå†…å®¹:")
        for idx, row in progress_df.iterrows():
            print(f"  ç§‘ç›®: {row['subject']}, æœªå­¦ç¿’: {row['unlearned_pct']:.1f}%, å­¦ç¿’ä¸­: {row['learned_pct'] - row['mastered_pct']:.1f}%, ç¿’å¾—æ¸ˆã¿: {row['mastered_pct']:.1f}%")
        
        # æœªå­¦ç¿’éƒ¨åˆ†ï¼ˆè–„ã„ã‚°ãƒ¬ãƒ¼ - è¦–èªæ€§å‘ä¸Šï¼‰
        unlearned_values = progress_df['unlearned_pct'].tolist()
        print(f"[DEBUG] æœªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {unlearned_values}")
        
        fig.add_trace(go.Bar(
            name='æœªå­¦ç¿’',
            y=progress_df['subject'],
            x=unlearned_values,
            orientation='h',
            marker_color='#BDBDBD',
            text=[f"{pct:.0f}%" if pct >= 10 else "" for pct in unlearned_values],
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>æœªå­¦ç¿’: %{x:.1f}%<extra></extra>'
        ))
        
        # å­¦ç¿’æ¸ˆã¿ï¼ˆæœªç¿’å¾—ï¼‰éƒ¨åˆ†ï¼ˆè¦–èªæ€§ã®é«˜ã„é’è‰²ï¼‰
        learning_pct = progress_df['learned_pct'] - progress_df['mastered_pct']
        # æœ€å°è¡¨ç¤ºå¹…ã‚’é©ç”¨
        learning_values = [max(pct, min_visible_width) if pct > 0 else pct for pct in learning_pct]
        fig.add_trace(go.Bar(
            name='å­¦ç¿’ä¸­',
            y=progress_df['subject'],
            x=learning_values,
            orientation='h',
            marker_color='#42A5F5',
            text=[f"{pct:.0f}%" if pct >= 5 else "" for pct in learning_pct],  # å…ƒã®å€¤ã§ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>å­¦ç¿’ä¸­: %{customdata:.1f}%<extra></extra>',
            customdata=learning_pct  # å…ƒã®å€¤ã‚’ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿æŒ
        ))
        
        # ç¿’å¾—æ¸ˆã¿éƒ¨åˆ†ï¼ˆé”æˆæ„Ÿã®ã‚ã‚‹ç·‘è‰²ï¼‰
        mastered_values = [max(pct, min_visible_width) if pct > 0 else pct for pct in progress_df['mastered_pct']]
        fig.add_trace(go.Bar(
            name='ç¿’å¾—æ¸ˆã¿',
            y=progress_df['subject'],
            x=mastered_values,
            orientation='h',
            marker_color='#4CAF50',
            text=[f"{pct:.0f}%" if pct >= 5 else "" for pct in progress_df['mastered_pct']],  # å…ƒã®å€¤ã§ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
            textposition='inside',
            hovertemplate='<b>%{y}</b><br>ç¿’å¾—æ¸ˆã¿: %{customdata:.1f}%<extra></extra>',
            customdata=progress_df['mastered_pct']  # å…ƒã®å€¤ã‚’ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿æŒ
        ))
        
        fig.update_layout(
            title={
                'text': "ç§‘ç›®åˆ¥é€²æ—çŠ¶æ³ï¼ˆå„ç§‘ç›®100%åŸºæº–ï¼‰",
                'x': 0,  # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å·¦å¯„ã›
                'xanchor': 'left'
            },
            xaxis_title="é€²æ—ç‡ (%)",
            yaxis_title="ç§‘ç›®",
            barmode='stack',
            height=max(600, len(progress_df) * 40),  # ç§‘ç›®æ•°ã«å¿œã˜ã¦é«˜ã•èª¿æ•´ï¼ˆæœ€å°600pxã€ã‚ˆã‚Šå¤§ããï¼‰
            width=None,  # å¹…ã‚’è‡ªå‹•èª¿æ•´
            xaxis=dict(range=[0, 105], tickformat='.0f', ticksuffix='%'),
            yaxis=dict(
                automargin=True, 
                tickmode='linear',
                side='left',  # Yè»¸ãƒ©ãƒ™ãƒ«ã‚’å·¦å´ã«é…ç½®
                categoryorder='total ascending'  # é€²æ—ç‡é †ã«ä¸¦ã³æ›¿ãˆ
            ),
            legend=dict(
                orientation="h", 
                yanchor="bottom", 
                y=1.02, 
                xanchor="left",  # å‡¡ä¾‹ã‚’å·¦å¯„ã›
                x=0
            ),
            margin=dict(l=200, r=50, t=100, b=50),  # ãƒãƒ¼ã‚¸ãƒ³ã‚’èª¿æ•´
            showlegend=True,
            plot_bgcolor='rgba(0,0,0,0)',  # é€æ˜èƒŒæ™¯
            paper_bgcolor='rgba(0,0,0,0)',  # é€æ˜èƒŒæ™¯
            font=dict(size=12)  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        )
        
        # æ¨ªæ£’ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºï¼ˆå·¦å¯„ã›ã§é«˜ã•ä¸­å¤®é…ç½®ï¼‰
        print(f"[DEBUG] ã‚°ãƒ©ãƒ•åˆ†æã‚¿ãƒ–ã§æ¨ªæ£’ã‚°ãƒ©ãƒ•è¡¨ç¤ºé–‹å§‹")
        print(f"[DEBUG] figã®å‹: {type(fig)}")
        print(f"[DEBUG] progress_dfç§‘ç›®æ•°: {len(progress_df)}")
        print(f"[DEBUG] figã®dataæ•°: {len(fig.data)}")
        print(f"[DEBUG] figã®é«˜ã•: {fig.layout.height}")
        
        # Streamlitã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ˜ç¤ºçš„ã«ä½œæˆã—ã¦ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
        with st.container():
            st.subheader("ğŸ“Š ç§‘ç›®åˆ¥é€²æ—çŠ¶æ³")
            try:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ã‚’ä½¿ç”¨
                import time
                chart_key = f"subject_progress_chart_{int(time.time())}"
                
                # ã‚°ãƒ©ãƒ•ãŒé©åˆ‡ã«ä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                if fig and fig.data and len(fig.data) > 0:
                    st.plotly_chart(fig, use_container_width=True, key=chart_key)
                    print(f"[DEBUG] Plotlyãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºæˆåŠŸ (key: {chart_key})")
                else:
                    print(f"[WARNING] figãŒç©ºã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨ç¤º")
                    raise Exception("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    
            except Exception as chart_error:
                print(f"[ERROR] Plotlyãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {chart_error}")
                st.warning(f"è©³ç´°ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {chart_error}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Streamlitæ¨™æº–ã®ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
                st.subheader("ğŸ“Š ç§‘ç›®åˆ¥é€²æ—çŠ¶æ³ï¼ˆç°¡æ˜“è¡¨ç¤ºï¼‰")
                try:
                    # progress_dfã‹ã‚‰ç°¡æ˜“ãƒãƒ£ãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                    chart_data = progress_df[['subject', 'å­¦ç¿’ä¸­', 'ç¿’å¾—æ¸ˆã¿']].set_index('subject')
                    chart_data.columns = ['å­¦ç¿’ä¸­(%)', 'ç¿’å¾—æ¸ˆã¿(%)']
                    st.bar_chart(chart_data)
                    print(f"[DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºæˆåŠŸ")
                except Exception as fallback_error:
                    print(f"[ERROR] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {fallback_error}")
                    st.error("ã‚°ãƒ©ãƒ•è¡¨ç¤ºæ©Ÿèƒ½ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºæ©Ÿèƒ½ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚")
        
        print(f"[DEBUG] ã‚°ãƒ©ãƒ•åˆ†æã‚¿ãƒ–ã§æ¨ªæ£’ã‚°ãƒ©ãƒ•è¡¨ç¤ºå®Œäº†")
        
        # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã¯éè¡¨ç¤ºï¼ˆUIãŒç…©é›‘ã«ãªã‚‹ãŸã‚ï¼‰
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©³ç´°ã‚’çŸ¥ã‚ŠãŸã„å ´åˆã¯ã‚°ãƒ©ãƒ•ã®ãƒ›ãƒãƒ¼æƒ…å ±ã§ååˆ†
        
    except Exception as e:
        print(f"[ERROR] ç§‘ç›®åˆ¥é€²æ—ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        st.error(f"ç§‘ç›®åˆ¥é€²æ—ã‚°ãƒ©ãƒ•ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        st.write("**ãƒ‡ãƒãƒƒã‚°æƒ…å ±:**")
        st.write(f"- filtered_df ã®è¡Œæ•°: {len(filtered_df)}")
        if not filtered_df.empty:
            st.write(f"- åˆ©ç”¨å¯èƒ½ãªç§‘ç›®: {list(filtered_df['subject'].unique())}")
            st.write(f"- ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ: {dict(filtered_df['level'].value_counts())}")
        else:
            st.write("- filtered_df ãŒç©ºã§ã™")
        
        # ç°¡æ˜“ç‰ˆã®ã‚°ãƒ©ãƒ•ã‚’è©¦ã™
        if not filtered_df.empty and 'subject' in filtered_df.columns and 'level' in filtered_df.columns:
            st.write("**ç°¡æ˜“ç‰ˆã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã—ã¾ã™:**")
            try:
                # ã‚·ãƒ³ãƒ—ãƒ«ãªç§‘ç›®åˆ¥é›†è¨ˆ
                subject_counts = filtered_df.groupby(['subject', 'level']).size().unstack(fill_value=0)
                if not subject_counts.empty:
                    st.bar_chart(subject_counts)
                else:
                    st.info("è¡¨ç¤ºå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            except Exception as simple_error:
                st.error(f"ç°¡æ˜“ç‰ˆã‚°ãƒ©ãƒ•ã‚‚è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {simple_error}")
        st.info("ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
    
    # å­¦ç¿’è¨˜éŒ² - UserDataExtractorå¼·åŒ–ç‰ˆ
    st.markdown("##### å­¦ç¿’ã®è¨˜éŒ²")
    
    # UserDataExtractorã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    if enhanced_analytics.get('evaluation_logs'):
        evaluation_logs = enhanced_analytics['evaluation_logs']
        
        # é«˜ç²¾åº¦ãªæ—¥åˆ¥å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        daily_study = defaultdict(lambda: {'count': 0, 'correct': 0, 'avg_quality': 0})
        today = datetime.datetime.now()
        ninety_days_ago = today - datetime.timedelta(days=90)
        
        quality_sum = defaultdict(int)
        
        for log in evaluation_logs:
            try:
                # ã‚ˆã‚Šå®‰å…¨ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ¼ã‚¹
                timestamp = log['timestamp']
                if isinstance(timestamp, str):
                    if 'T' in timestamp:
                        # ISOå½¢å¼
                        timestamp_str = timestamp.split('.')[0] if '.' in timestamp else timestamp
                        log_datetime = datetime.datetime.fromisoformat(timestamp_str)
                    else:
                        # é€šå¸¸å½¢å¼
                        log_datetime = datetime.datetime.fromisoformat(timestamp[:19])
                else:
                    # datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    log_datetime = timestamp
                    
                if log_datetime >= ninety_days_ago:
                    date_str = log_datetime.date().isoformat()
                    daily_study[date_str]['count'] += 1
                    quality = log.get('quality', 0)
                    quality_sum[date_str] += quality
                    if quality >= 3:
                        daily_study[date_str]['correct'] += 1
            except:
                continue
        
        # å¹³å‡è©•ä¾¡ã‚’è¨ˆç®—
        for date_str in daily_study:
            if daily_study[date_str]['count'] > 0:
                daily_study[date_str]['avg_quality'] = quality_sum[date_str] / daily_study[date_str]['count']
        
        if daily_study:
            try:
                study_df = pd.DataFrame([
                    {
                        'æ—¥ä»˜': date_str,
                        'å­¦ç¿’å›æ•°': data['count'],
                        'æ­£è§£æ•°': data['correct'],
                        'æ­£è§£ç‡': (data['correct'] / data['count'] * 100) if data['count'] > 0 else 0,
                        'å¹³å‡è©•ä¾¡': data['avg_quality']
                    }
                    for date_str, data in daily_study.items()
                ])
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if study_df.empty:
                    st.info("å­¦ç¿’è¨˜éŒ²ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return
                
                study_df['æ—¥ä»˜'] = pd.to_datetime(study_df['æ—¥ä»˜'])
                study_df = study_df.sort_values('æ—¥ä»˜')
                
                print(f"[DEBUG] study_df shape: {study_df.shape}")
                print(f"[DEBUG] study_df columns: {study_df.columns.tolist()}")
                
                # 2ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
                col1, col2 = st.columns(2)
                
                with col1:
                    # å­¦ç¿’å›æ•°ã‚°ãƒ©ãƒ•
                    try:
                        fig1 = px.bar(
                            study_df, 
                            x='æ—¥ä»˜', 
                            y='å­¦ç¿’å›æ•°',
                            title='å­¦ç¿’å›æ•°æ¨ç§»ï¼ˆéå»90æ—¥ï¼‰',
                            color='å­¦ç¿’å›æ•°',
                            color_continuous_scale='Blues'
                        )
                        fig1.update_traces(hovertemplate='<b>%{x|%Y-%m-%d}</b><br>å­¦ç¿’å›æ•°: %{y}å•<extra></extra>')
                        fig1.update_layout(coloraxis_showscale=False, height=300)
                        st.plotly_chart(fig1, use_container_width=True)
                    except Exception as e:
                        print(f"[ERROR] å­¦ç¿’å›æ•°ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                        st.error(f"å­¦ç¿’å›æ•°ã‚°ãƒ©ãƒ•ã®ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒ©ãƒ•
                        st.bar_chart(study_df.set_index('æ—¥ä»˜')['å­¦ç¿’å›æ•°'])
                
                with col2:
                    # æ­£è§£ç‡ã‚°ãƒ©ãƒ•
                    try:
                        fig2 = px.line(
                            study_df, 
                            x='æ—¥ä»˜', 
                            y='æ­£è§£ç‡',
                            title='æ­£è§£ç‡æ¨ç§»ï¼ˆéå»90æ—¥ï¼‰',
                            line_shape='spline'
                        )
                        fig2.update_traces(hovertemplate='<b>%{x|%Y-%m-%d}</b><br>æ­£è§£ç‡: %{y:.1f}%<extra></extra>')
                        fig2.update_layout(height=300)
                        fig2.update_traces(line_color='#FF6B6B')
                        st.plotly_chart(fig2, use_container_width=True)
                    except Exception as e:
                        print(f"[ERROR] æ­£è§£ç‡ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                        st.error(f"æ­£è§£ç‡ã‚°ãƒ©ãƒ•ã®ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒ©ãƒ•
                        st.line_chart(study_df.set_index('æ—¥ä»˜')['æ­£è§£ç‡'])
                        
            except Exception as df_error:
                print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆã‚¨ãƒ©ãƒ¼: {df_error}")
                st.error(f"å­¦ç¿’è¨˜éŒ²ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {df_error}")
                st.write("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                st.write(f"daily_study keys: {list(daily_study.keys())[:5]}...")
                if daily_study:
                    first_key = list(daily_study.keys())[0]
                    st.write(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {daily_study[first_key]}")
            
            # å¼·åŒ–ã•ã‚ŒãŸçµ±è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
            col1, col2, col3, col4 = st.columns(4)
            total_days = len(study_df)
            total_sessions = study_df['å­¦ç¿’å›æ•°'].sum()
            avg_daily = study_df['å­¦ç¿’å›æ•°'].mean()
            avg_accuracy = study_df['æ­£è§£ç‡'].mean()

            with col1:
                st.metric("å­¦ç¿’æ—¥æ•°", f"{total_days}æ—¥", help="éå»90æ—¥é–“ã®å®Ÿç¸¾")
            with col2:
                st.metric("ç·å­¦ç¿’å›æ•°", f"{total_sessions}å›", help="éå»90æ—¥é–“ã®å®Ÿç¸¾")
            with col3:
                st.metric("1æ—¥å¹³å‡", f"{avg_daily:.1f}å›", help="éå»90æ—¥é–“ã®å­¦ç¿’æ—¥å¹³å‡")
            with col4:
                st.metric("å¹³å‡æ­£è§£ç‡", f"{avg_accuracy:.1f}%", help="éå»90æ—¥é–“ã®å¹³å‡")
        else:
            st.info("å­¦ç¿’è¨˜éŒ²ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯
        daily_study = defaultdict(int)
        today = datetime.datetime.now()
        ninety_days_ago = today - datetime.timedelta(days=90)

        for _, row in filtered_df.iterrows():
            history = row.get('history', [])
            if isinstance(history, list):
                for entry in history:
                    if isinstance(entry, dict) and 'timestamp' in entry:
                        try:
                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†
                            timestamp = entry['timestamp']
                            if hasattr(timestamp, 'date'):
                                entry_datetime = timestamp
                            else:
                                # ã‚ˆã‚Šå®‰å…¨ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ¼ã‚¹
                                try:
                                    if 'T' in str(timestamp):
                                        # ISOå½¢å¼
                                        timestamp_str = str(timestamp).split('.')[0] if '.' in str(timestamp) else str(timestamp)
                                        entry_datetime = datetime.datetime.fromisoformat(timestamp_str)
                                    else:
                                        # é€šå¸¸å½¢å¼
                                        entry_datetime = datetime.datetime.fromisoformat(str(timestamp)[:19])
                                except Exception as e:
                                    print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (search_page line 939): {e}")
                                    continue
                            
                            # 90æ—¥ä»¥å†…ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿é›†è¨ˆ
                            if entry_datetime >= ninety_days_ago:
                                date_str = entry_datetime.date().isoformat()
                                daily_study[date_str] += 1
                        except:
                            continue

        if daily_study:
            study_df = pd.DataFrame(list(daily_study.items()), columns=['æ—¥ä»˜', 'å­¦ç¿’å›æ•°'])
            study_df['æ—¥ä»˜'] = pd.to_datetime(study_df['æ—¥ä»˜'])
            study_df = study_df.sort_values('æ—¥ä»˜')
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªæ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
            fig = px.bar(
                study_df, 
                x='æ—¥ä»˜', 
                y='å­¦ç¿’å›æ•°',
                title='éå»90æ—¥é–“ã®å­¦ç¿’è¨˜éŒ²',
                color='å­¦ç¿’å›æ•°',
                color_continuous_scale='OrRd'
            )
            
            fig.update_traces(hovertemplate='<b>%{x|%Y-%m-%d}</b><br>å­¦ç¿’å›æ•°: %{y}å•<extra></extra>')
            fig.update_layout(
                xaxis_title='æ—¥ä»˜',
                yaxis_title='å­¦ç¿’å›æ•°',
                coloraxis_showscale=False,
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # åŸºæœ¬çµ±è¨ˆ
            col1, col2, col3, col4 = st.columns(4)
            total_days = len(study_df)
            total_sessions = study_df['å­¦ç¿’å›æ•°'].sum()
            avg_daily = study_df['å­¦ç¿’å›æ•°'].mean()
            max_daily = study_df['å­¦ç¿’å›æ•°'].max()

            with col1:
                st.metric("å­¦ç¿’æ—¥æ•°", f"{total_days}æ—¥", help="éå»90æ—¥é–“ã®å®Ÿç¸¾")
            with col2:
                st.metric("ç·å­¦ç¿’å›æ•°", f"{total_sessions}å›", help="éå»90æ—¥é–“ã®å®Ÿç¸¾")
            with col3:
                st.metric("1æ—¥å¹³å‡", f"{avg_daily:.1f}å›", help="éå»90æ—¥é–“ã®å­¦ç¿’æ—¥å¹³å‡")
            with col4:
                st.metric("æœ€å¤§å­¦ç¿’å›æ•°", f"{max_daily}å›", help="éå»90æ—¥é–“ã®æœ€å¤§å€¤")
        else:
            st.info("å­¦ç¿’è¨˜éŒ²ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ãƒ¬ãƒ™ãƒ«åˆ¥åˆ†å¸ƒ
    st.markdown("##### å­¦ç¿’ãƒ¬ãƒ™ãƒ«åˆ¥åˆ†å¸ƒ")
    
    level_counts = filtered_df['level'].value_counts()
    level_counts = level_counts.reindex(LEVEL_ORDER, fill_value=0)
    
    try:
        # Plotlyè£½ã®æ£’ã‚°ãƒ©ãƒ•
        fig = px.bar(
            x=level_counts.index, 
            y=level_counts.values,
            title="å­¦ç¿’ãƒ¬ãƒ™ãƒ«åˆ¥åˆ†å¸ƒ",
            color=level_counts.index,
            color_discrete_map=LEVEL_COLORS
        )
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    except:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.bar_chart(level_counts)

def render_question_list_tab_perfect(filtered_df: pd.DataFrame, analysis_target: str = "å›½è©¦"):
    """
    å•é¡Œãƒªã‚¹ãƒˆã‚¿ãƒ– - ç¸¦é•·ãƒªã‚¹ãƒˆå½¢å¼ã§ã®å…¨é¢åˆ·æ–°
    ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã«åˆè‡´ã™ã‚‹å…¨ã¦ã®å•é¡Œã‚’ä¸€è¦§è¡¨ç¤º
    """
    st.subheader("å•é¡Œãƒªã‚¹ãƒˆ")
    
    if filtered_df.empty:
        st.info("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å•é¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # åˆ†æå¯¾è±¡ã®è¡¨ç¤º
    target_text = "å­¦å£«è©¦é¨“å•é¡Œ" if analysis_target in ["å­¦å£«è©¦é¨“", "å­¦å£«è©¦é¨“å•é¡Œ"] else "æ­¯ç§‘å›½è©¦å•é¡Œ"
    st.caption(f"å¯¾è±¡: {target_text}")
    
    # 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã«åŸºã¥ãä¿®æ­£ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§å …ç‰¢ãªã‚½ãƒ¼ãƒˆé–¢æ•°ã«ç½®ãæ›ãˆ
    def get_natural_sort_key(q_id):
        """
        ã‚·ãƒ³ãƒ—ãƒ«ã§å …ç‰¢ãªå•é¡Œç•ªå·ã‚½ãƒ¼ãƒˆã‚­ãƒ¼ç”Ÿæˆé–¢æ•°
        å›½è©¦å•é¡Œï¼ˆ118A1ï¼‰ã¨å­¦å£«è©¦é¨“å•é¡Œï¼ˆG24-1-1-A-1ï¼‰ã®ä¸¡æ–¹ã«å¯¾å¿œ
        """
        import re
        q_id = str(q_id)
        
        # å­¦å£«è©¦é¨“å•é¡Œï¼ˆGå§‹ã¾ã‚Šï¼‰ã®å ´åˆ
        if q_id.startswith('G'):
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: G22-1-1-A-1 å½¢å¼
            match1 = re.match(r'G(\d+)-(\d+)-(\d+)-([A-Z])-(\d+)', q_id)
            if match1:
                year, term, session, section, number = match1.groups()
                return (1, int(year), int(term), int(session), 0, section, int(number))
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: G23-2-A-67 å½¢å¼
            match2 = re.match(r'G(\d+)-(\d+)-([A-Z])-(\d+)', q_id)
            if match2:
                year, term, section, number = match2.groups()
                return (1, int(year), int(term), 999, 0, section, int(number))
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: G22-1å†-C-75 å½¢å¼ï¼ˆå†è©¦é¨“ï¼‰
            match3 = re.match(r'G(\d+)-(\d+)å†-([A-Z])-(\d+)', q_id)
            if match3:
                year, term, section, number = match3.groups()
                return (1, int(year), int(term), 1000, 0, section, int(number))
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³4: æ—§å½¢å¼ G97A1
            match4 = re.match(r'G(\d+)([A-Z])(\d+)', q_id)
            if match4:
                year, section, number = match4.groups()
                return (1, int(year), 0, 0, 0, section, int(number))
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return (1, 0, 0, 9999, 0, 'Z', 9999)
        else:
            # å›½è©¦å•é¡Œã®å ´åˆï¼š118A1, 95C40 ãªã©
            match = re.match(r'(\d+)([A-Z]?)(\d+)', q_id)
            if match:
                year, section, number = match.groups()
                section = section if section else 'A'
                return (0, int(year), 0, 0, 0, section, int(number))
            else:
                # æ•°å€¤ã®ã¿ã®å ´åˆ
                num_match = re.search(r'(\d+)', q_id)
                if num_match:
                    return (0, 0, 0, 0, 0, 'A', int(num_match.group(1)))
                else:
                    return (0, 0, 0, 9999, 0, 'Z', 9999)
    
    # 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã«åŸºã¥ãä¿®æ­£ï¼štry-exceptã§ã‚½ãƒ¼ãƒˆå‡¦ç†ã‚’å›²ã‚€
    try:
        sorted_df = filtered_df.copy()
        sorted_df['sort_key'] = sorted_df['id'].apply(get_natural_sort_key)
        sorted_df = sorted_df.sort_values('sort_key').drop('sort_key', axis=1)
    except Exception as e:
        # 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã«åŸºã¥ãä¿®æ­£ï¼šãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆæ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆï¼‰
        print(f"[WARNING] ã‚½ãƒ¼ãƒˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€æ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
        sorted_df = filtered_df.sort_values('id')
    
    # --- â–¼ ã“ã“ã‹ã‚‰ãŒä¿®æ­£éƒ¨åˆ†ï¼šãƒªã‚¹ãƒˆå½¢å¼è¡¨ç¤º ---
    
    # 1. è¡¨ç¤ºåˆ¶é™ã‚’æ’¤å»ƒã—ã€å…¨ä»¶ã‚’è¡¨ç¤ºå¯¾è±¡ã¨ã™ã‚‹
    display_df = sorted_df
    total_count = len(display_df)
    st.write(f"è¡¨ç¤ºå¯¾è±¡: {total_count}å•")

    # 2. ãƒ«ãƒ¼ãƒ—å‡¦ç†ã§ãƒªã‚¹ãƒˆé …ç›®ã‚’ç”Ÿæˆ
    for _, row in display_df.iterrows():
        level = row['level']
        color = LEVEL_COLORS.get(level, "#757575")
        q_id = row['id']
        # å®Ÿéš›ã®ç§‘ç›®åã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ¨™æº–åŒ–ã¯è¡Œã‚ãªã„ï¼‰
        actual_subject = row['subject']
        
        # 3. HTMLã¨CSSã§ãƒªã‚¹ãƒˆé …ç›®ã‚’ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
        list_item_html = f"""
        <div style="
            border-left: 5px solid {color}; 
            padding: 5px 10px; 
            margin: 3px 0; 
            border-radius: 3px;
            display: flex;
            align-items: center;
        ">
            <span style="
                color: {color}; 
                font-weight: bold; 
                width: 80px; 
                flex-shrink: 0;
            ">{level}</span>
            <span style="font-weight: 500;">{q_id}</span>
            <span style="color: #666; margin-left: 15px; font-size: 0.9em;">{actual_subject}</span>
        </div>
        """
        st.markdown(list_item_html, unsafe_allow_html=True)

def render_keyword_search_tab_perfect(analysis_target: str):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¿ãƒ– - å®Œå…¨å†ç¾ç‰ˆ
    æ¤œç´¢æ©Ÿèƒ½ã€çµ±è¨ˆè¡¨ç¤ºã€çµæœãƒªã‚¹ãƒˆè¡¨ç¤ºã€PDFç”Ÿæˆæ©Ÿèƒ½ã‚’å«ã‚€
    """
    st.subheader("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")

    # 1. æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
    col1, col2 = st.columns([4, 1])
    with col1:
        keyword = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", key="search_keyword", 
                               placeholder="ä¾‹ï¼šæ ¹ç®¡æ²»ç™‚ã€ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆã€å’¬åˆ")
    with col2:
        shuffle_results = st.checkbox("æ¤œç´¢çµæœã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«", key="shuffle_search")

    if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", key="execute_search", type="primary", use_container_width=True):
        if keyword:
            # 2. æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ (è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å¯¾è±¡)
            search_results = []
            
            for question in ALL_QUESTIONS:
                q_number = question.get('number', '')
                
                # analysis_targetãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
                if analysis_target in ["å›½è©¦", "å›½è©¦å•é¡Œ"] and q_number.startswith('G'):
                    continue
                if analysis_target in ["å­¦å£«è©¦é¨“", "å­¦å£«è©¦é¨“å•é¡Œ"] and not q_number.startswith('G'):
                    continue
                
                # è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
                searchable_text = [
                    question.get('question', ''),  # æ­£ã—ã„ã‚­ãƒ¼
                    question.get('subject', ''),
                    q_number,
                    str(question.get('choices', [])),
                    question.get('answer', ''),
                    question.get('explanation', '')  # è§£èª¬ã‚‚æ¤œç´¢å¯¾è±¡ã«è¿½åŠ 
                ]
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                combined_text = ' '.join(searchable_text).lower()
                if keyword.lower() in combined_text:
                    search_results.append(question)
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³é©ç”¨
            if shuffle_results:
                random.shuffle(search_results)
            
            # æ¤œç´¢çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state["search_results"] = search_results
            st.session_state["search_query"] = keyword
            st.session_state["search_analysis_target"] = analysis_target
            st.session_state["search_shuffled"] = shuffle_results
        else:
            st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # 3. æ¤œç´¢çµæœè¡¨ç¤º
    if "search_results" in st.session_state:
        results = st.session_state["search_results"]
        query = st.session_state.get("search_query", "")
        search_type = st.session_state.get("search_analysis_target", "å…¨ä½“")
        is_shuffled = st.session_state.get("search_shuffled", False)

        if results:
            # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            shuffle_info = "ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«æ¸ˆã¿ï¼‰" if is_shuffled else "ï¼ˆé †ç•ªé€šã‚Šï¼‰"
            st.success(f"ã€Œ{query}ã€ã§{len(results)}å•è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆ{search_type}ï¼‰{shuffle_info}")

            # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            subjects = set(q.get('subject', '') for q in results)
            
            # å¹´åº¦ç¯„å›²ã®è¨ˆç®—ï¼ˆextract_year_from_question_numberä½¿ç”¨ï¼‰
            years = [extract_year_from_question_number(q.get("number", "")) for q in results]
            valid_years = [y for y in years if y is not None]
            year_range = f"{min(valid_years)}-{max(valid_years)}" if valid_years else "ä¸æ˜"
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ãƒ’ãƒƒãƒˆæ•°", len(results))
            with col2:
                st.metric("é–¢é€£ç§‘ç›®æ•°", len(subjects))
            with col3:
                st.metric("å¹´åº¦ç¯„å›²", year_range)

            # æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
            st.subheader("æ¤œç´¢çµæœ")
            for i, q in enumerate(results[:20]):  # 20ä»¶ã«åˆ¶é™
                q_number = q.get('number', 'N/A')
                subject = q.get('subject', 'æœªåˆ†é¡')
                
                # å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã¨å±¥æ­´ã‚’å–å¾—
                cards = st.session_state.get('cards', {})
                card = cards.get(q_number, {})
                level = calculate_card_level(card)
                
                # st.expanderã‚¿ã‚¤ãƒˆãƒ«
                with st.expander(f"â— {q_number} - {subject}"):
                    # å­¦ç¿’ãƒ¬ãƒ™ãƒ«ï¼ˆæœ€ä¸Šéƒ¨ï¼‰
                    st.markdown(f"**å­¦ç¿’ãƒ¬ãƒ™ãƒ«:** {level}")
                    
                    # å•é¡Œæ–‡ï¼ˆçœç•¥è¡¨ç¤ºï¼‰
                    question_text = q.get('question', '')
                    if len(question_text) > 100:
                        st.markdown(f"**å•é¡Œ:** {question_text[:100]}...")
                    else:
                        st.markdown(f"**å•é¡Œ:** {question_text}")
                    
                    # é¸æŠè‚¢ï¼ˆçœç•¥è¡¨ç¤ºï¼‰
                    choices = q.get('choices', [])
                    if choices:
                        st.markdown("**é¸æŠè‚¢:**")
                        for j, choice in enumerate(choices):
                            if isinstance(choice, dict):
                                choice_text = choice.get('text', str(choice))
                            else:
                                choice_text = str(choice)
                            
                            if len(choice_text) > 50:
                                st.markdown(f"  {chr(65 + j)}. {choice_text[:50]}...")
                            else:
                                st.markdown(f"  {chr(65 + j)}. {choice_text}")
                    
                    # æ­£è§£
                    answer = q.get('answer', '')
                    if answer:
                        st.markdown(f"**æ­£è§£:** {answer}")
                    
                    # å­¦ç¿’å±¥æ­´
                    history = card.get('history', [])
                    n = card.get('n', 0)
                    if not history:
                        st.markdown("**å­¦ç¿’å±¥æ­´:** ãªã—")
                    else:
                        st.markdown(f"**å­¦ç¿’å±¥æ­´:** {len(history)}å›")
                        st.markdown(f"**æ¼”ç¿’å›æ•°:** {n}å›")
                        # æœ€æ–°ã®å­¦ç¿’è¨˜éŒ²ã‚’è¡¨ç¤º
                        if len(history) > 0:
                            latest = history[-1]
                            timestamp = latest.get('timestamp', '')
                            quality = latest.get('quality', 0)
                            if timestamp:
                                try:
                                    if hasattr(timestamp, 'strftime'):
                                        time_str = timestamp.strftime('%Y-%m-%d %H:%M')
                                    else:
                                        # ã‚ˆã‚Šå®‰å…¨ãªæ–‡å­—åˆ—å‡¦ç†
                                        try:
                                            if 'T' in str(timestamp):
                                                # ISOå½¢å¼
                                                timestamp_str = str(timestamp).split('.')[0] if '.' in str(timestamp) else str(timestamp)
                                                parsed_time = datetime.datetime.fromisoformat(timestamp_str)
                                                time_str = parsed_time.strftime('%Y-%m-%d %H:%M')
                                            else:
                                                time_str = str(timestamp)[:16]
                                        except:
                                            time_str = "ä¸æ˜"
                                    st.markdown(f"ã€€æœ€æ–°: {time_str} (è©•ä¾¡: {quality})")
                                except:
                                    st.markdown(f"ã€€æœ€æ–°: (è©•ä¾¡: {quality})")

            # 4. PDFç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
            st.markdown("#### ğŸ“„ PDFç”Ÿæˆ")
            colA, colB = st.columns(2)
            
            with colA:
                if st.button("ğŸ“„ PDFã‚’ç”Ÿæˆ", key="pdf_generate_button"):
                    with st.spinner("PDFã‚’ç”Ÿæˆä¸­... é«˜å“è³ªãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ãŸã‚æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"):
                        # å‚ç…§å…ƒapp.pyã®PDFç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ã«ç§»æ¤
                        assets, per_q_files = _gather_images_for_questions(results)
                        latex_source = export_questions_to_latex_tcb_jsarticle(results, right_label_fn=lambda q: q.get('subject', ''))
                        
                        # ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç½®æ›
                        for i, files in enumerate(per_q_files, start=1):
                            block = _image_block_latex(files)
                            latex_source = latex_source.replace(rf"%__IMAGES_SLOT__{i}__", block)

                        pdf_bytes, log = compile_latex_to_pdf(latex_source, assets=assets)

                        if pdf_bytes:
                            ts = datetime.datetime.now().strftime("%Y%m%d_%H%M")
                            st.session_state["pdf_bytes_for_download"] = pdf_bytes
                            st.session_state["pdf_filename_for_download"] = f"search_results_{ts}.pdf"
                            st.success("âœ… PDFç”Ÿæˆå®Œäº†ï¼å³ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                        else:
                            st.error("âŒ PDFç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                            # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                            if "pdf_bytes_for_download" in st.session_state:
                                del st.session_state["pdf_bytes_for_download"]
                            with st.expander("ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"):
                                st.code(log or "ãƒ­ã‚°ã¯ã‚ã‚Šã¾ã›ã‚“", language="text")
            
            with colB:
                # st.session_stateã«PDFãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æ´»æ€§åŒ–
                if "pdf_bytes_for_download" in st.session_state and st.session_state["pdf_bytes_for_download"]:
                    file_size_kb = len(st.session_state["pdf_bytes_for_download"]) / 1024
                    st.download_button(
                        label="ğŸ“¥ PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=st.session_state["pdf_bytes_for_download"],
                        file_name=st.session_state["pdf_filename_for_download"],
                        mime="application/pdf",
                        use_container_width=True,
                        type="primary",  # ç›®ç«‹ã¤ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
                        help=f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_kb:.1f} KB"
                    )
                else:
                    # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒœã‚¿ãƒ³ã‚’éæ´»æ€§çŠ¶æ…‹ã§è¡¨ç¤º
                    st.button("ğŸ“¥ PDFã‚’DL", disabled=True, use_container_width=True)
        else:
            if query:
                st.warning(f"ã€Œ{query}ã€ã«è©²å½“ã™ã‚‹å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã—ã¦ãã ã•ã„")

# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…¬é–‹é–¢æ•°ã¨ã—ã¦è¨­å®š
def main():
    """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    render_search_page()

if __name__ == "__main__":
    main()
